{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea3_Propuesta.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "mo95-O7JSJgV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/MARCA-Color.jpg\" title=\"Title text\" width=\"50%\" height=\"50%\" />\n",
        "\n",
        "\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales II-2018 </h1>\n",
        "\n",
        "<H3 align='center'> Tarea 3 - Redes Recurrentes y Autoencoders </H3>\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "\n",
        "\n",
        "\n",
        "**Temas**  \n",
        "* Diseño e implementación de Redes Neuronales Recurrentes (RNN).\n",
        "* Regularización en Redes Recurrentes.\n",
        "* Autoencoders tradicionales y sus aplicaciones\n",
        "\n",
        "** Formalidades **  \n",
        "* Equipos de trabajo de: 2 personas (*cada uno debe estar en condiciones de realizar una presentación y discutir sobre cada punto del trabajo realizado*)\n",
        "* Se debe preparar una presentación de 20 minutos. Presentador será elegido aleatoriamente.\n",
        "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
        "* Fecha de entrega y discusión: 21 de Diciembre (sin posibilidad de extensiones)\n",
        "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<margarita.bugueno.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<cvalle@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea3-INF395-II-2018] \n",
        "\n",
        "<hr style=\"height:2px;border:none\"/>\n",
        "\n",
        "La tarea se divide en dos secciones:\n",
        "\n",
        "[1.](#primero) RNN sobre texto    \n",
        "[2.](#segundo) Autoencoder en MNIST  \n",
        "\n",
        "\n",
        "### **Nota Importante:**  \n",
        "Para esta actividad **si es que no se cuenta con GPU** se recomienda utilizar el entorno virtual de __[Colaboratory - Google](https://colab.research.google.com/)__* . Así, podrá programar en la nube con recursos elevados y luego descargar el Jupyter Notebook y entregarlo en modo Informe. \n"
      ]
    },
    {
      "metadata": {
        "id": "O94QbCzxSoOa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"primero\"></a>\n",
        "## 1. RNN sobre texto\n",
        "\n",
        "Hoy en dı́a, una aplicación relevante de las redes neuronales recurrentes es el modelamiento de texto y lenguaje natural. En esta sección abordaremos el problema de procesar sentencias de texto, proporcionadas por GMB (*Groningen Meaning Bank*), para reconocimiento de entidades y tagger. En específico, trabajaremos con el dataset proprocionado a través de __[Kaggle](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)__, que está compuesto por más de un millón de palabras, a fin de realizar predicciones sobre distintas tareas del tipo *many to many* y *many to one*.\n",
        "\n",
        "<img src=\"https://i.stack.imgur.com/b4sus.jpg\" width=\"70%\" />\n",
        "\n",
        "\n",
        "Descargue los datos de la página de Kaggle y cárguelos mediante *pandas*.\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "df_ner = pd.read_csv(\"./entity-annotated-corpus/ner.csv\", encoding =\"cp1252\", error_bad_lines=False)\n",
        "df_ner.dropna(inplace=True)\n",
        "```\n",
        "\n",
        "\n",
        "> a) En esta primera instancia trabajaremos con la tarea de realizar un NER *tag* (**Named Entity Recognition**) sobre cada una de las palabras en las sentencias que se nos presenta en los datos. Esta tarea es del tipo *many to many*, es decir, la entrada es una secuencia y la salida es una secuencia, sin *shift*, por lo que necesitaremos una estructura de red adecuada a ésto. En primer lugar extraiga las columnas que utilizaremos del dataset **¿Por qué es conveniente utilizar *lemma* en vez de la palabra misma *word*?**\n",
        "```python\n",
        "dataset = df_ner.loc[:,[\"lemma\",\"word\",\"pos\",\"tag\",\"prev-iob\"]]\n",
        "```\n",
        "Luego de esto cree una estructura que contendrá todas las sentencias u oraciones y otra estructura que contendrá los las etiquetas (*tags*), esto es un arreglo de arreglos de *lemmas* y un arreglo de arreglos de *tags* respectivamente. **¿Cuales son las dimensiones de ambas estructuras? ¿Cada dato de ejemplo tiene las mismas dimensiones que el resto?**\n",
        "```python\n",
        "dataX,dataY = [],[]\n",
        "lemmas,labels = set(), set()  #uniques\n",
        "for fila in dataset.values:\n",
        "    if fila[-1]==\"__START1__\": \n",
        "        dataX.append(np.asarray(sentence))\n",
        "        dataY.append(np.asarray(labels_sentence))\n",
        "        sentence= []\n",
        "        label_sentence = []\n",
        "    lemmas.add(fila[0])\n",
        "    labels.add(fila[3])\n",
        "    sentence.append(fila[0])#add lemma\n",
        "    labels_sentence.append(fila[3]) #TAG\n",
        "dataX = np.asarray(dataX[1:]) #data to  array\n",
        "dataY = np.asarray(dataY[1:])\n",
        "```    \n",
        "\n",
        "> b) Estudie la distribución del largo de los textos a procesar. Estudie también la frecuencia con la que aparecen las palabras en todo el dataset. **¿Se observa una ley Zipf?**[[1]](#refs) Realice un gráfico de la cantidad de datos por clase. Comente.\n",
        "\n",
        "> c) Es necesario transformar los textos para que puedan ser entregados apropiadamente a la red, por lo que será necesario crear una función que codifique cada posible *lemma* a un número y cada posible *tag* a otro número, utilice esta función sobre las sentencias y *tags* ya generados. **Mida cual es el largo máximo de entre todas las sentencias, la cantidad de *lemmas* y etiquetas**. Además de esto, debido al largo distinto de las sentencias se deberá **realizar *padding* para estandarizar el largo**, considere algun carácter especial para codificar el espacio en blanco que luego se le deberá rellenar, por ejemplo si el largo máximo es de 4 y se tiene la sentencia \"the rocket\" codificada como [32,4] será necesario agregar un *lemma* que codificado significará el fin de la sentencia \"the rocket *ENDPAD ENDPAD*\" que codificado quedará como [32,4,*0, 0*].\n",
        "```python\n",
        "...#add fullfill lemma and tag to the dictionary\n",
        "lemma2idx = {w: i for i, w in enumerate(lemmas)} #Converting text to numbers\n",
        "lab2idx = {t: i for i, t in enumerate(labels)}\n",
        "dataX = [[lemma2idx[lemma] for lemma in sentence ] for sentence in dataX]\n",
        "dataY = [[lab2idx[ner] for ner in ner_tags ] for ner_tags in dataY]\n",
        "n_lemmas = len(lemmas)\n",
        "n_labels = len(labels)\n",
        "```\n",
        "\n",
        "> d) Realice el *padding* anteriormente mencionado, decida sobre qué le parece más conveniente al rellenar con un valor especial respecto al cómo funciona una red recurrente y cómo funciona su memoria **¿Al principio o al final de la sentencia?** Comente\n",
        "```python\n",
        "from keras.preprocessing import sequence\n",
        "X = sequence.pad_sequences(dataX,maxlen=max_input_lenght,padding='post' or 'pre',value=lemma2idx[\"yourspecialcharacter\"]) \n",
        "y = sequence.pad_sequences(dataY,maxlen=max_input_lenght,padding='post' or 'pre',value=lab2idx[\"endtagger\"])\n",
        "```\n",
        "\n",
        "> e) Para el poder entregar una clasificación sobre los distintos *tags* es necesario tranformarlos a *one hot vectors*, debido a que están codificados en números enteros, esto resultará en un arreglo tridimensional con la cantidad de ejemplos, la cantidad máxima de palabras y la cantidad de posibles *tags*. Luego de esto cree los conjuntos de entrenamiento y de prueba con el código a continuación **¿Cuáles son las dimensiones de entrada y salida de cada conjunto?** Comente\n",
        "```python\n",
        "from keras.utils import to_categorical\n",
        "y = np.asarray([to_categorical(i, num_classes=n_labels) for i in y])\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=22)\n",
        "```\n",
        "\n",
        "\n",
        "> f) Defina una red neuronal recurrente *many to many* con compuertas LSTM para aprender a *tagear* la entidad en el texto, entrene y evalúe su desempeño sobre ambos conjuntos. Esta red debe procesar la secuencia de *lemmas* rellenados (o sin rellenar) y entregar el *tag* a cada uno de los *lemmas*, por lo que la salida de la red no es un vector como anteriormente se ha trabajado, sino que tiene una dimensión extra la cual es debido a que en cada instante de tiempo se necesita entregar un *output*. Como los *lemmas* corresponden a datos esencialmente categóricos, o al menos discretos, es necesario generar una representación vectorial de ellas. La primera capa de la red a construir debe por lo tanto incluir una transformación entrenable desde el espacio de representación original (discreto) a ${\\rm I\\!R}^{d}$ , con $d$ la dimensionalidad del *embedding*. **Comente sobre los cambios que sufre un dato al ingresar a la red y la cantidad de parámetros de la red**.\n",
        "```python\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, Dense, Dropout\n",
        "embedding_vector = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=n_lemmas, output_dim=embedding_vector, input_length=max_input_lenght))\n",
        "model.add(LSTM(units=100,return_sequences=True))\n",
        "model.add(Dense(n_labels, activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=128)\n",
        "```\n",
        "Para evaluar su modelo utilice una métrica adecauda para el desbalance presente entre las clases como identificó en el punto b).\n",
        "```python\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"F1 score on test: \", f1_score(y_test, model.predict(X_test,verbose=0))\n",
        "```\n",
        "\n",
        "> g) Varı́e la dimensionalidad del embedding inicial y determine si **aumenta o disminuye el error de clasificación**. Comente.\n",
        "\n",
        "> h) Use *Dropout* para entrenar la LSTM. **¿El *Dropout* mejora el desempeño de la red?** Señale cuáles podrı́an ser las causas del comportamiento observado.\n",
        "```python\n",
        "from keras.layers import Dropout\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=n_lemmas, output_dim=embedding_vector, input_length=max_input_lenght))\n",
        "model.add(LSTM(units=100,return_sequences=True)) #or recurrent_dropout=0.2\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(n_labels, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=128)\n",
        "```\n",
        "\n",
        "> i) Algunos autores señalan la importante dependencia que existe en texto, no solo con las palabras anteriores, sino que con las que siguen.**Mejore la red definida en f) utilizando una red neuronal recurrente Bidireccional**, es decir, con recurrencia en ambas direcciones sobre la secuencia de *lemmas* de entrada. Comente **cuál debiera ser la forma correcta de usar el parámetro *merge_mode*** (concatenar, multiplicar, sumar o promediar) para este caso. Además comente las transformaciones que sufre el patrón de entrada al pasar por las capas. **¿Mejora o empeora el desempeño?** Analice.\n",
        "```python\n",
        "from keras.layers import Bidirectional\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=n_lemmas, output_dim=embedding_vector, input_length=max_input_lenght))\n",
        "layer_lstm = LSTM(units=100,return_sequences=True)\n",
        "model.add(Bidirectional(layer_lstm,merge_mode=choose))\n",
        "model.add(Dense(n_labels, activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=128)\n",
        "```\n",
        "\n",
        "> j) Recientemente se ha implementado la capa de *Masking* en las redes recurrentes en *keras*, lo cual podría traer gran ayuda gracias al *padding* que se realiza con el símbolo especial definido. **Entrene la red definida en f) y compare al utilizar esta funcionalidad de enmascarar el valor 0 en este caso para el *default* de la capa *embedding***, comente sobre las curvas de entrenamiento y los tiempos de entrenamiento.\n",
        "```python\n",
        "lemma2idx[\"yourspecialcharacter\"] = 0 #should be zero.. if not correct it\n",
        "embedding_vector = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=n_lemmas, output_dim=embedding_vector, input_length=max_input_lenght,mask_zero=True))\n",
        "...\n",
        "```\n",
        "\n",
        "> k) En base a lo experimentado, **intente mejorar el desempeño de las redes encontradas**, ya sea utilizando y combinando las distintas variaciones que se hicieron en los distintos ítemes, como bien alguna mejora en el pre-proceso de los datos (largo de secuencia, el tipo de *padding* o alguna otra), agregar mayor profundidad, variar el número de unidades/neuronas, utilizando otra *gate* de recurrencia (GRU o Vanilla/Simple), en https://keras.io/layers/recurrent/,  entre otras.\n",
        "\n",
        "\n",
        "> l) Utilice la red entranda anteriormente, **se espera que sea la mejor de esta sección**, y **muestre las predicciones**, el *NER tager*, sobre algún ejemplo de pruebas, comente.  \n",
        "```python\n",
        "p = model.predict(np.array([X_test[i]]))\n",
        "p = np.argmax(p, axis=-1)\n",
        "print(\"{:15}: {}\".format(\"Lemma\", \"Pred\"))\n",
        "for w,pred in zip(X_test[i],p[0]):\n",
        "    print(\"{:15}: {}\".format(lemmas[w],labels[pred]))\n",
        "```\n",
        "\n",
        "Ahora utilizaremos el mismo dataset para realizar una aplicación más conocida hoy en día que es el autocompletar texto, esto es, predecir la siguiente palabra de una sentencia basada en las palabras anteriores de la misma, por lo que la red que utilizaremos es del tipo *many to one*.  \n",
        "Debido a lo extenso del vocabulario es bastante complejo hacer un modelo que prediga una palabra dentro de las millones que pueden haber, por lo que, **trabajaremos a nivel de carácter, en donde las posibilidades (posibles clases) son mucho menores**.\n",
        "\n",
        "> m) **Carge las palabras del dataset** ¿Por qué no los *lemmas*? y cree el corpus con el cual se trabajará, además de crear la codificación de caracteres a números. Esto se presenta en el código a continuación además de crear la estrucutura de los datos con los que se va a trabajar (sub sentencias del corpus original). **Utilice el tamaño del *corpus* que le acomode a la memoria de su computador**.\n",
        "```python\n",
        "dataset = df_ner.loc[:,[\"word\",\"lemma\"]]\n",
        "text = ' '.join(dataset[\"word\"]).lower() #corpus\n",
        "null_character = \"*\"\n",
        "chars = [null_character]+sorted(list(set(text)))\n",
        "print('total chars:', len(chars))\n",
        "char_indices = {c: i for i, c in enumerate(chars)}\n",
        "indices_char = {i: c for i, c in enumerate(chars)}\n",
        "maxlen = 40 # cut the text in semi-redundant sequences of maxlen characters\n",
        "step = 5 \n",
        "sentences = []\n",
        "next_chars = []\n",
        "size = int(len(text)*0.2) #solo un 20% del corpus\n",
        "for i in range(0, size - maxlen, step):\n",
        "    sentences.append(null_character+text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "```\n",
        "\n",
        "> n) **Procese las sentencias para así tenerlas codificadas en números que van a representar los carácteres**, tal cual se realizó en c) con los *lemmas*, **lo mismo para las etiquetas**. Además de esto deberá realizar el *padding* correspondiente al comienzo de la sentencia, esto es para que la red aprenda cuando venga una frase mas corta de lo entrenado, este símbolo siignificará que no hay información. **Transforme las etiquetas a *one hot vector*** como se realizó en c) y **defina la red** similar a la presentada en f), con un *embedding* seguido de una capa recurrente GRU y la capa de clasificación. Aprovechese de la implementación más rápida de GRU respaldada por __[CuDNN](https://developer.nvidia.com/cudnn)__, una librería de CUDA (NVIDIA)[[2]](#refs) para *Deep Neural Network*. \n",
        "```python\n",
        "dataX = [[char_indices[char] for char in sentence ] for sentence in sentences]\n",
        "dataY = [char_indices[char] for char in next_chars]\n",
        "...#dataX pad sequence padding='pre'\n",
        "...#dataY to categorical with num_classes=len(chars)\n",
        "from keras.layers import CuDNNGRU,GRU\n",
        "embedding_vector = 16\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(chars), output_dim=embedding_vector, input_length=maxlen+1))#\n",
        "model.add(CuDNNGRU(units=512,return_sequences=False)) #or GRU\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        ">  o) Entrene la red con las funciones que se presentan a continuación que mostrarán el cómo va la tarea de autocompletar texto en cada *epoch*, generando una sentencia completa de 400 carácteres *aleatoriamente* a partir de una semilla *random*. Entrene solo durante 25 *epochs*, a los 15 ya debería comenzar a generar palabras y sonar mas coherente **¿Cuál es la técnica para predecir el siguiente carácter?**\n",
        "```python\n",
        "def predict_next_char(model, sentence):\n",
        "    \"\"\"Predict the next character from the current one\"\"\"    \n",
        "    x_pred = [char_indices[null_character]]+[char_indices[char] for char in sentence]\n",
        "    x_pred = sequence.pad_sequences([x_pred], maxlen=maxlen+1,padding='pre',value=char_indices[null_character])\n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    next_index = np.random.choice(len(chars), p=preds) #take a sample\n",
        "    return indices_char[next_index]\n",
        "import random,sys\n",
        "def on_epoch_end(epoch, logs):\n",
        "    \"\"\"Function invoked at end of each epoch. Prints generated text.\"\"\"\n",
        "    print('\\n----- Generating text after Epoch: %d' % epoch)\n",
        "    start_index = random.randint(0, size - maxlen - 1)\n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(sentence)\n",
        "    for i in range(400):\n",
        "        next_char = predict_next_char(model, sentence0)\n",
        "        sentence = sentence[1:] + next_char #for next character\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    return\n",
        "from keras.callbacks import LambdaCallback\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "model.fit(X, y,batch_size=256,epochs=25, callbacks=[print_callback])\n",
        "```\n",
        "\n",
        "> p) Verifique la calidad de la red entrenada, cargando el modelo si es que lo guardó o directamente, entregando una predicción sobre una semilla inicial que usted entregue. **Observe y comente cualitativamente sobre qué pasa cuando la predicción del siguiente carácter fuese de manera determinista, tomando el máximo valor de entre las predicciones.**\n",
        "```python\n",
        "sentence = \"it is \"\n",
        "print('----- Generating with seed: \"' + sentence + '\"')\n",
        "sys.stdout.write(sentence)\n",
        "for i in range(400):\n",
        "    next_char = predict_next_char(model, sentence)\n",
        "    sentence = sentence[1:] + next_char \n",
        "    sys.stdout.write(next_char)\n",
        "    sys.stdout.flush()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "LMReuo3lntIf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"segundo\"></a>\n",
        "## 2. Autoencoders (AEs) en MNIST\n",
        "\n",
        "\n",
        "Como se ha discutido en clases, las RBM’s y posteriormente los AE’s (redes no supervisadas) fueron un componente crucial en el desarrollo de los modelos que entre 2006 y 2010 vigorizaron el área de las redes neuronales artificiales con logros notables de desempeño en diferentes tareas de aprendizaje automático. En esta sección aprenderemos a utilizar el más sencillo de estos modelos: un autoencoder o AE. Consideraremos tres aplicaciones clásicas: reducción de dimensionalidad, *denoising* y pre-entrenamiento. Con este objetivo en mente, utilizaremos un dataset denominado MNIST[[3]](#refs). Se trata de una colección de 70000 imágenes de 28 $\\times$ 28 pixeles correspondientes a dígitos manuscritos (números entre 0 y 9). En su versión tradicional, la colección se encuentra separada en dos subconjuntos: uno de entrenamiento de 60000 imágenes y otro de test de 10000 imágenes. La tarea consiste en construir un programa para que aprenda a identificar correctamente el dı́gito representado en la imagen\n",
        "\n",
        "\n",
        "> a) Escriba el código que **cargue los datos** desde el repositorio de keras, normalice las imágenes de modo que los pixeles queden en [0, 1], transforme las imágenes en vectores ($\\in {\\rm I\\!R}^{784}$) y devuelva tres subconjuntos disjuntos: uno de entrenamiento, uno de validación y uno de pruebas. Construya el conjunto de validación de la manera que estime conveniente, éste debe contar con $nval = 5000$ imágenes.\n",
        "```python\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255. #and x_test\n",
        "...#Define here your validation set\n",
        "```\n",
        "\n",
        "### 2.1 Reducción de dimensionalidad\n",
        "Para esta primera sección, gracias a la simplicidad del problema tratado, se experimentará con un autoencoder tradicional (*feed forward*) en donde las capas de éste sean densas. Para esto se re estructurarán los datos de entradas en forma de vector, es decir la matriz de 28 $\\times$ 28 pasa a ser un vector de 784 componentes.\n",
        "\n",
        "```python\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "...#other sets \"x\"\n",
        "```\n",
        "\n",
        "Una de las aplicaciones tı́picas de un AE es reducción de dimensionalidad, es decir, implementar una transformación $\\phi:{\\rm I\\!R}^d \\rightarrow {\\rm I\\!R}^{d'}$ de objetos representados originalmente por $d$ atributos en una nueva representación de $d'$ atributos, de modo tal que se preserve lo mejor posible la “información” original. Obtener tal representación es útil desde un punto de vista computacional (compresión) y estadı́stico (permite construir modelos con un menor número de parámetros libres). Un AE es una técnica de reducción de dimensionalidad no supervisada porque no hace uso de información acerca de las clases a las que pertenecen los datos de entrenamiento.  \n",
        "\n",
        "> a) Entrene un AE básico (1 capa escondida) para generar una representación de MNIST en $d'$= 2, 8, 32, 64 dimensiones. **Justifique la elección de la función de pérdida a utilizar y del criterio de entrenamiento en general**. Determine el porcentaje de compresión obtenido y el error de reconstrucción en cada caso. **¿Mejora el resultado si elegimos una función de activación *ReLU* para el *Encoder*? ¿Podrı́a y/o corresponde utilizar ésta activación en el *Decoder*?**\n",
        "```python\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "input_img = Input(shape=(784,))\n",
        "encoded = Dense(32, activation='sigmoid')(input_img)\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "autoencoder = Model(input=input_img, output=decoded)\n",
        "encoder = Model(input=input_img, output=encoded)\n",
        "encoded_input = Input(shape=(32,))\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "decoder = Model(inputs=encoded_input, outputs=decoder_layer(encoded_input))\n",
        "autoencoder.compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
        "autoencoder.fit(x_train,x_train,epochs=50,batch_size=32,validation_data=(x_val,x_val))\n",
        "autoencoder.save('basic_autoencoder_768x32.h5')\n",
        "...#save other stuffs if you want\n",
        "```\n",
        "\n",
        "> b) Compare visualmente la reconstrucción que logra hacer el *autoencoder* desde la representación en ${\\rm I\\!R}^{d'}$ para algunas imágenes del conjunto de pruebas. **Determine si la percepción visual se corresponde con el error de reconstrucción observada**. Comente.\n",
        "```python\n",
        "from keras.models import load_model\n",
        "autoencoder = load_model('basic_autoencoder_768x32.h5')\n",
        "...#load other stuff\n",
        "encoded_test = encoder.predict(x_test)\n",
        "decoded_test = decoder.predict(encoded_test)\n",
        "import matplotlib.pyplot as plt\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    j = np.random.randint(0,len(x_test))\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[j].reshape(28, 28),cmap='gray')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_test[j].reshape(28, 28),cmap='gray')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "> c) Para verificar la calidad de la representación obtenida, implemente el clasificador denominado $kNN$ (k-nearest neighbor): dada una imagen $x$, el clasificador busca las k = 10 imágenes de entrenamiento más similares (de acuerdo a una distancia, e.g. euclidiana) y predice como clase, la etiqueta más popular entre las imágenes cercanas. **Mida el error de pruebas** obtenido construyendo este clasificador sobre la data reducida a través del *autoencoder* comparando con la representación reducida obtenida vía PCA (una técnica clásica de reducción de dimensionalidad) utilizando el mismo número de dimensiones $d'$= 2, 4, 8, 16, 32. Considere tanto el error de reconstrucción como el desempeño en clasificación , además de comparar los tiempos medios de predicción en ambos escenarios **¿La representación generada por el *autoencoder* logra generalizar?**\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "pca = PCA(n_components=d)\n",
        "pca.fit(x_train)\n",
        "pca_train = pca.transform(x_train)\n",
        "pca_test = pca.transform(x_test)\n",
        "...#AUTOENCODER\n",
        "encoded_train = encoder.predict(x_train)\n",
        "encoded_test = encoder.predict(x_test)\n",
        "...#CLASIFICATION\n",
        "clf = KNeighborsClassifier(10)\n",
        "clf.fit(pca_train, y_train)\n",
        "print 'Classification Accuracy PCA %.2f' % clf.score(pca_test,y_test)\n",
        "clf = KNeighborsClassifier(10)\n",
        "clf.fit(encoded_train, y_train)\n",
        "print 'Classification Accuracy %.2f' % clf.score(encoded_test,y_test)\n",
        "```\n",
        "\n",
        "> d) Modifique el *autoencoder* básico construido en (a) para implementar un deep autoencoder (*deep AE*), es decir, un autoencoder con al menos dos capas ocultas. **Demuestre experimentalmente** que este *autoencoder* puede mejorar la compresión obtenida por PCA utilizando el mismo número de dimensiones $d'$ . Experimente con $d'$ =2, 4, 8, 16 y distintas profundidades ($L \\in [2,4]$). Considere en esta comparación tanto el error de reconstrucción como el desempeño en clasificación (vı́a kNN) de cada representación. Comente.\n",
        "```python\n",
        "target_dim = 2 #try other and do a nice plot\n",
        "input_img = Input(shape=(784,))\n",
        "encoded1 = Dense(1000, activation='relu')(input_img)\n",
        "encoded2 = Dense(500, activation='relu')(encoded1)\n",
        "encoded3 = Dense(250, activation='relu')(encoded2)\n",
        "encoded4 = Dense(target_dim, activation='relu')(encoded3)\n",
        "decoded4 = Dense(250, activation='relu')(encoded4)\n",
        "decoded3 = Dense(500, activation='relu')(encoded3)\n",
        "decoded2 = Dense(1000, activation='relu')(decoded3)\n",
        "decoded1 = Dense(784, activation='sigmoid')(decoded2)\n",
        "autoencoder = Model(input=input_img, output=decoded1)\n",
        "encoder = Model(input=input_img, output=encoded3)\n",
        "autoencoder.compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
        "autoencoder.fit(x_train,x_train,epochs=40,batch_size=32,validation_data=(x_val,x_val))\n",
        "autoencoder.save('my_autoencoder_768x1000x500x250x2.h5')\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "pca = PCA(n_components=target_dim)\n",
        "pca.fit(x_train)\n",
        "```\n",
        "\n",
        "> e) Elija algunas de las representaciones aprendidas anteriormente ($d>2$) y visualı́celas usando la herramienta *TSNE* disponible en la librerı́a *sklearn*. **Compare cualitativamente el resultado con aquel obtenido usando PCA** con el mismo número de componentes ($d>2$). Finalmente **grafique una representación** generada por un autoencoder directamente ($d=2$), comente.\n",
        "```python\n",
        "nplot=5000 #warning: mind your memory!\n",
        "encoded_train = encoder.predict(x_train[:nplot])\n",
        "from sklearn.manifold import TSNE\n",
        "model = TSNE(n_components=2, random_state=0)\n",
        "encoded_train = model.fit_transform(encoded_train)\n",
        "plt.figure(figsize=(10, 10))\n",
        "colors={0:'b',1:'g',2:'r',3:'c',4:'m',5:'y',6:'k',7:'orange',8:'darkgreen',9:'maroon'}\n",
        "markers={0:'o',1:'+',2: 'v',3:'<',4:'>',5:'^',6:'s',7:'p',8:'*',9:'x'}\n",
        "for idx in xrange(0,nplot):\n",
        "    label = y_train[idx]\n",
        "    line = plt.plot(encoded_train[idx][0], encoded_train[idx][1],\n",
        "        color=colors[label], marker=markers[label], markersize=6)\n",
        "pca_train = pca.transform(x_train)\n",
        "encoded_train = pca_train[:nplot]\n",
        "... #plot PCA\n",
        "... #plot AE (d=2) without TSNE\n",
        "encoded_train = encoder2d.predict(x_train[:nplot]) #Autoencoder with d=2\n",
        "plt.figure(figsize=(10, 10))\n",
        "colors={0:'b',1:'g',2:'r',3:'c',4:'m',5:'y',6:'k',7:'orange',8:'darkgreen',9:'maroon'}\n",
        "markers={0:'o',1:'+',2: 'v',3:'<',4:'>',5:'^',6:'s',7:'p',8:'*',9:'x'}\n",
        "for idx in xrange(0,nplot):\n",
        "    label = y_train[idx]\n",
        "    line = plt.plot(encoded_train[idx][0], encoded_train[idx][1],\n",
        "        color=colors[label], marker=markers[label], markersize=6)\n",
        "```\n",
        "\n",
        "> f) Cuando el problema se torna más difícil es necesario complejizar el modelo. Modifique el autoencoder construido en (a) para trabajar directamente sobre las imágenes de MNIST, sin tratarlas como vectores de 784 atributos, sino como matrices de tamaño $1\\times28\\times28$. Es posible lograr este objetivo utilizando capas convolucionales para definir el *Encoder* y capas con **convoluciones transpuesta** en el *Decoder*, comente como sufre las transformaciones el patrón de entrada. Compare la calidad de la representación reducida obtenida por el nuevo autoencoder con aquella obtenida anteriormente utilizando el mismo número de dimensiones. Comente.\n",
        "```python\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) #modify for th dim ordering\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "from keras.layers import *\n",
        "input_img = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2DTranspose(16, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2DTranspose(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.summary()\n",
        "```\n",
        "\n",
        "### 2.2 Denoising\n",
        "Como se ha discutido en clases, un *denoising autoencoder* (dAE)[[4]](#refs) es escencialmente un autoencoder entrenado para reconstruir ejemplos parcialmente corruptos. Varios autores han demostrado que mediante esta modificación simple es posible obtener representaciones más robustas y significativas que aquellas obtenidas por un AE básico. En esta sección exploraremos la aplicación más “natural” o “directa” del método.\n",
        "\n",
        "> a) **Genere artificialmente una versión corrupta de las imágenes en MNIST** utilizando el siguiente modelo de ruido (masking noise): si $ x \\in {\\rm I\\!R}^d $ es una de las imágenes originales, la versión ruidosa $\\tilde{x}$ se obtiene como $\\tilde{x} = x \\odot \\xi$ donde $\\odot$ denota el producto de Hadamard (componente a componente) y $\\xi \\in {\\rm I\\!R}^d$ es un vector aleatorio binario con componentes *Ber(p)* independientes.\n",
        "```python\n",
        "from numpy.random import binomial\n",
        "noise_level = 0.1\n",
        "noise_mask = binomial(n=1,p=noise_level,size=x_train.shape)\n",
        "noisy_x_train = x_train*noise_mask\n",
        "noise_mask = binomial(n=1,p=noise_level,size=x_val.shape)\n",
        "noisy_x_val = x_val*noise_mask\n",
        "noise_mask = binomial(n=1,p=noise_level,size=x_test.shape)\n",
        "noisy_x_test = x_test*noise_mask\n",
        "```\n",
        "\n",
        "> b) Entrene un autoencoder para reconstruir las imágenes corruptas generadas en el ı́tem anterior. **Mida el error de reconstrucción y evalúe cualitativamente** (visualización de la imagen corrupta y reconstruida) el resultado para un subconjunto representativo de imágenes. **Experimente diferentes valores de *p* en el rango (0, 1).**\n",
        "```python\n",
        "...# DEFINE YOUR AUTOENCODER AS BEFORE\n",
        "autoencoder.fit(noisy_x_train, x_train, epochs=40, batch_size=32, validation_data=(noisy_x_val, x_val))\n",
        "```\n",
        "\n",
        "> c) Utilice la representación reducida, genera por el *denoising AE*, para **medir el desempeño en clasificación** (vı́a kNN como en la sección anterior). Comente.\n",
        "\n",
        "> d) Diseñe otra manera de generar imágenes corruptas del dataset MNIST, por ejemplo algún tipo de ruido, sea creativo. **Mida el error de reconstrucción y evalúe cualitativamente** (visualización de la imagen corrupta y reconstruida) el resultado para un subconjunto representativo de imágenes\n",
        "\n",
        "### 2.3 *Similarity reconstruct*\n",
        "\n",
        "En esta sección se explorará una forma diferente de implementar un *autoencoder* que es utilizar la arquitectura del *autoencoder* pero no para reconstruir el mismo dato, sino que para reconstruir un dato similar. En este caso la similaridad estará dada por las clases de los datos.\n",
        "\n",
        "> a) Genere pares de objetos $(objeto,similar)$ con 10 datos similares al dato \"objeto\", para ésto utilice la función que se provee a continuación sobre los primeros (1000 a 2000) datos de entrenamiento. **Visualice los nuevos datos generados y la relación que se produce entre los pares de objetos**.\n",
        "```python\n",
        "def similarity_data(X,Y,sim=10):\n",
        "    index_classes = [ np.where(Y==number)[0] for number in range(10)]\n",
        "    new_X = np.zeros((1,X.shape[1]))\n",
        "    simi_X = np.zeros((1,X.shape[1]))\n",
        "    for x,y in zip(X,Y):\n",
        "        similarities = index_classes[y]\n",
        "        sample_sim = np.random.choice(similarities,sim)\n",
        "        new_X = np.concatenate((new_X, np.tile(x,(sim,1))),axis=0)\n",
        "        simi_X = np.concatenate((simi_X, X[sample_sim]),axis=0)\n",
        "    return new_X[1:],simi_X[1:]\n",
        "data, data_sim = similarity_data(x_train[:2000],y_train[:2000])\n",
        "...#visualize data\n",
        "```\n",
        "\n",
        "> b) Escoga algunas de las arquitecturas ya experimentadas hasta este punto de la actividad y entrénela para enfrentarla a éste problema **¿La función de pérdida se mantiene?**\n",
        "```python\n",
        "autoencoder.fit(data,data_sim,epochs=50,batch_size=32,validation_split=0.2)\n",
        "```\n",
        "\n",
        "> c) **Visualice lo que genera el *autoencoder* dado una imagen de entrada**. Además **visualice, con la herramienta TSNE, los *embedding*/representación reducida** que se producen en el *encoder*.\n",
        "```python\n",
        "autoencoder.predict(data)\n",
        "embeddings = encoder.predict(data) #project this with TSNE\n",
        "```\n",
        "\n",
        "\n",
        "### 2.4 Pre-*training*\n",
        "\n",
        "En esta sección utilizaremos un AE para pre-entrenar redes profundas. Como hemos discutido en clases, el efecto esperado es regularizar el modelo, posicionando el modelo de partida en una buena zona del espacio de parámetros.\n",
        "\n",
        "> a) Construya y entrene una red FF para clasificar las imágenes de MNIST. Utilice SGD básico con tasa de aprendizaje fija $\\eta = 0.01$, momentum $m=0.9$ y no más de 50 *epochs*. Para empezar, utilice una arquitectura $768 \\times 1000 \\times 1000 \\times 10$ y **funciones de activación sigmoidales**. **Determine error de clasificación alcanzado por el modelo en el conjunto de test.**\n",
        "```python\n",
        "from keras.utils import to_categorical\n",
        "Y_train = to_categorical(y_train, 10)\n",
        "Y_test = to_categorical(y_test, 10)\n",
        "from keras.models import Sequential\n",
        "model = Sequential()\n",
        "model.add(Dense(1000, activation='sigmoid', input_shape=(784,)))\n",
        "model.add(Dense(1000, activation='sigmoid'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.summary()\n",
        "optimizer_ = SGD(lr=0.01, momentum=0.9)\n",
        "model.compile(optimizer=optimizer_,loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, Y_train,nb_epoch=50, batch_size=25,shuffle=True, validation_data=(x_val, Y_val))\n",
        "model.save('ReluNet-768x1000x1000x10-NFT-50epochs.h5')\n",
        "```\n",
        "\n",
        "> b) Construya y entrene una red neuronal profunda para clasificar las imágenes de MNIST utilizando la arquitectura propuesta en (a) y pre-entrenando los pesos de cada capa mediante un autoencoder básico. Proceda en modo clásico, es decir, entrenando en modo no supervisado una capa a la vez y tomando como input de cada nivel la representación (entrenada) obtenida en el nivel anterior. Después del entrenamiento efectúe un entrenamiento supervisado convencional (*fine-tunning*). **Compare los resultados de clasificación sobre el conjunto de pruebas con aquellos obtenidos en (a), sin pre-entrenamiento. Evalúe también los resultados antes del *fine-tunning*.** Comente.\n",
        "```python\n",
        "from keras.datasets import mnist\n",
        "... ## Load and preprocess MNIST as usual\n",
        "...###AUTOENCODER 1\n",
        "input_img1 = Input(shape=(784,))\n",
        "encoded1 = Dense(n_hidden_layer1,activation=activation_layer1)(input_img1)\n",
        "decoded1 = Dense(784, activation=decoder_activation_1)(encoded1)\n",
        "autoencoder1 = Model(inputs=input_img1, outputs=decoded1)\n",
        "encoder1 = Model(inputs=input_img1, outputs=encoded1)\n",
        "autoencoder1.compile(optimizer=optimizer_, loss=loss_)\n",
        "autoencoder1.fit(x_train, x_train, nb_epoch=epochs_, batch_size=batch_size_,shuffle=True, validation_data=(x_val, x_val))\n",
        "encoded_input1 = Input(shape=(n_hidden_layer1,))\n",
        "autoencoder1.save('autoencoder_layer1.h5')\n",
        "encoder1.save('encoder_layer1.h5')\n",
        "...###AUTOENCODER 2\n",
        "x_train_encoded1 = encoder1.predict(x_train) #FORWARD PASS DATA THROUGH FIRST ENCODER\n",
        "x_val_encoded1 = encoder1.predict(x_val)\n",
        "x_test_encoded1 = encoder1.predict(x_test)\n",
        "input_img2 = Input(shape=(n_hidden_layer1,))\n",
        "encoded2 = Dense(n_hidden_layer2, activation=activation_layer2)(input_img2)\n",
        "decoded2 = Dense(n_hidden_layer2, activation=decoder_activation_2)(encoded2)\n",
        "autoencoder2 = Model(inputs=input_img2, outputs=decoded2)\n",
        "encoder2 = Model(inputs=input_img2, outputs=encoded2)\n",
        "autoencoder2.compile(optimizer=optimizer_, loss=loss_)\n",
        "autoencoder2.fit(x_train_encoded1,x_train_encoded1,nb_epoch=epochs_,batch_size=batch_size_,shuffle=True, validation_data=(x_val_encoded1, x_val_encoded1))\n",
        "encoded_input2 = Input(shape=(n_hidden_layer2,))\n",
        "autoencoder2.save('autoencoder_layer2.h5')\n",
        "encoder2.save('encoder_layer2.h5')\n",
        "...#FINE TUNNING\n",
        "from keras.models import Sequential\n",
        "model = Sequential()\n",
        "model.add(Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)))\n",
        "model.layers[-1].set_weights(autoencoder1.layers[1].get_weights())\n",
        "model.add(Dense(n_hidden_layer2, activation=activation_layer2))\n",
        "model.layers[-1].set_weights(autoencoder2.layers[1].get_weights())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(optimizer=optimizer_,loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, Y_train,nb_epoch=20, batch_size=25,\n",
        "shuffle=True, validation_data=(x_val, Y_val))\n",
        "model.save('Net-768x1000x1000x10-finetunned.h5')\n",
        "```\n",
        "\n",
        "> c) Repita usando funciones de **activación *tanh*. Comente**"
      ]
    },
    {
      "metadata": {
        "id": "b8wP3g1zHJMX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"refs\"></a>\n",
        "## Referencias\n",
        "[1] George Kingsley Zipf (1949), *Human behavior and the principle of least effort*, Addison-Wesley Press  \n",
        "[2] https://www.nvidia.es/object/cuda-parallel-computing-es.html  \n",
        "[3] http://yann.lecun.com/exdb/mnist/  \n",
        "[4] Vincent, P., Larochelle, H., Bengio, Y., & Manzagol, P. A. (2008, July). *Extracting and composing robust features with denoising autoencoders*. ACM."
      ]
    }
  ]
}