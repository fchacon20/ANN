{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import cross_validation\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "from keras import regularizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16242 entries, 0 to 16241\n",
      "Columns: 1278 entries, Unnamed: 0 to Eat\n",
      "dtypes: float64(1276), int64(2)\n",
      "memory usage: 158.4 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1267</th>\n",
       "      <th>1268</th>\n",
       "      <th>1269</th>\n",
       "      <th>1270</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>pubchem_id</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8139.041805</td>\n",
       "      <td>115.715266</td>\n",
       "      <td>22.445723</td>\n",
       "      <td>20.474191</td>\n",
       "      <td>18.529573</td>\n",
       "      <td>17.169350</td>\n",
       "      <td>15.816888</td>\n",
       "      <td>15.133152</td>\n",
       "      <td>14.471534</td>\n",
       "      <td>13.960759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>33107.484300</td>\n",
       "      <td>-11.178969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4698.182820</td>\n",
       "      <td>113.198503</td>\n",
       "      <td>8.659586</td>\n",
       "      <td>7.670481</td>\n",
       "      <td>6.485777</td>\n",
       "      <td>5.512560</td>\n",
       "      <td>4.179691</td>\n",
       "      <td>3.885091</td>\n",
       "      <td>3.503075</td>\n",
       "      <td>3.357136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.043869</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.032755</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.024472</td>\n",
       "      <td>23456.785147</td>\n",
       "      <td>3.659133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>2.906146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-23.245373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4068.250000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.969345</td>\n",
       "      <td>16.228071</td>\n",
       "      <td>15.165862</td>\n",
       "      <td>13.744092</td>\n",
       "      <td>13.653146</td>\n",
       "      <td>13.637784</td>\n",
       "      <td>12.759519</td>\n",
       "      <td>12.587359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12298.250000</td>\n",
       "      <td>-13.475805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8142.500000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.662511</td>\n",
       "      <td>18.631287</td>\n",
       "      <td>17.690729</td>\n",
       "      <td>16.020040</td>\n",
       "      <td>15.156646</td>\n",
       "      <td>13.848274</td>\n",
       "      <td>13.659233</td>\n",
       "      <td>13.652832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27731.500000</td>\n",
       "      <td>-10.835211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12207.750000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>21.132432</td>\n",
       "      <td>20.739496</td>\n",
       "      <td>18.712895</td>\n",
       "      <td>18.297501</td>\n",
       "      <td>17.639688</td>\n",
       "      <td>16.154918</td>\n",
       "      <td>15.499474</td>\n",
       "      <td>14.900585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55020.750000</td>\n",
       "      <td>-8.623903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16272.000000</td>\n",
       "      <td>388.023441</td>\n",
       "      <td>73.563510</td>\n",
       "      <td>66.269180</td>\n",
       "      <td>66.268891</td>\n",
       "      <td>66.268756</td>\n",
       "      <td>66.268196</td>\n",
       "      <td>66.264158</td>\n",
       "      <td>66.258487</td>\n",
       "      <td>66.258177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062225</td>\n",
       "      <td>0.061999</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.061534</td>\n",
       "      <td>0.059760</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.057834</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>74980.000000</td>\n",
       "      <td>-0.789513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0             0             1             2             3  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean    8139.041805    115.715266     22.445723     20.474191     18.529573   \n",
       "std     4698.182820    113.198503      8.659586      7.670481      6.485777   \n",
       "min        0.000000     36.858105      2.906146      0.000000      0.000000   \n",
       "25%     4068.250000     73.516695     17.969345     16.228071     15.165862   \n",
       "50%     8142.500000     73.516695     20.662511     18.631287     17.690729   \n",
       "75%    12207.750000     73.516695     21.132432     20.739496     18.712895   \n",
       "max    16272.000000    388.023441     73.563510     66.269180     66.268891   \n",
       "\n",
       "                  4             5             6             7             8  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean      17.169350     15.816888     15.133152     14.471534     13.960759   \n",
       "std        5.512560      4.179691      3.885091      3.503075      3.357136   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       13.744092     13.653146     13.637784     12.759519     12.587359   \n",
       "50%       16.020040     15.156646     13.848274     13.659233     13.652832   \n",
       "75%       18.297501     17.639688     16.154918     15.499474     14.900585   \n",
       "max       66.268756     66.268196     66.264158     66.258487     66.258177   \n",
       "\n",
       "           ...               1267          1268          1269          1270  \\\n",
       "count      ...       16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean       ...           0.000134      0.000133      0.003879      0.000131   \n",
       "std        ...           0.002728      0.002705      0.043869      0.002676   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "75%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "max        ...           0.062225      0.061999      0.500000      0.061534   \n",
       "\n",
       "               1271          1272          1273          1274    pubchem_id  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean       0.000129      0.002155      0.000127      0.001201  33107.484300   \n",
       "std        0.002633      0.032755      0.002594      0.024472  23456.785147   \n",
       "min        0.000000      0.000000      0.000000      0.000000      1.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000  12298.250000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000  27731.500000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000  55020.750000   \n",
       "max        0.059760      0.500000      0.057834      0.500000  74980.000000   \n",
       "\n",
       "                Eat  \n",
       "count  16242.000000  \n",
       "mean     -11.178969  \n",
       "std        3.659133  \n",
       "min      -23.245373  \n",
       "25%      -13.475805  \n",
       "50%      -10.835211  \n",
       "75%       -8.623903  \n",
       "max       -0.789513  \n",
       "\n",
       "[8 rows x 1278 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lectura de datos e informacion relevante de estos\n",
    "datos= pd.read_csv(\"roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.info()\n",
    "datos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "#Estas columnas se eliminan debido a que no son originarias de las muestras, son valores utilizados para mantener un orden y numeracion de los datos.\n",
    "#No contienen relación alguna con los valores.\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "X_val_scaled =  pd.DataFrame(scaler.transform(df_val),columns=df_val.columns)\n",
    "X_test_scaled =  pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "\n",
    "y_train_scaled = df_train.pop('Eat').values.reshape(-1,1)\n",
    "y_val_scaled = df_val.pop('Eat').values.reshape(-1,1)\n",
    "y_test_scaled = df_test.pop('Eat').values.reshape(-1,1)\n",
    "\n",
    "X_train_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_val_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_test_scaled.drop(columns=['Eat'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b)\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se obtienen los gradientes sin entrenar el modelo\n",
    "loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "listOfVariableTensors = model.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Capa 2')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#histograma de gradientes\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(141)\n",
    "plt.hist(evaluated_gradients[0])\n",
    "plt.title(\"Capa 1\")\n",
    "plt.subplot(142)\n",
    "plt.hist(evaluated_gradients[1])\n",
    "plt.title(\"Capa 1\")\n",
    "plt.subplot(143)\n",
    "plt.hist(evaluated_gradients[2])\n",
    "plt.title(\"Capa 2\")\n",
    "plt.subplot(144)\n",
    "plt.hist(evaluated_gradients[3])\n",
    "plt.title(\"Capa 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 1.3323 - val_loss: 0.5546\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.6046 - val_loss: 0.6562\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.4811 - val_loss: 0.3683\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.4028 - val_loss: 0.3059\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.3470 - val_loss: 0.2808\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.3009 - val_loss: 0.2404\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.2620 - val_loss: 0.2524\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.2389 - val_loss: 0.2265\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.2136 - val_loss: 0.2286\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.1935 - val_loss: 0.1978\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.1739 - val_loss: 0.2027\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.1577 - val_loss: 0.2260\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.1446 - val_loss: 0.1509\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.1320 - val_loss: 0.1327\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.1231 - val_loss: 0.1296\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.1140 - val_loss: 0.1358\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.1045 - val_loss: 0.1301\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.1004 - val_loss: 0.1060\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0937 - val_loss: 0.1051\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0864 - val_loss: 0.0991\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0819 - val_loss: 0.0928\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0780 - val_loss: 0.1175\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0754 - val_loss: 0.0870\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0732 - val_loss: 0.0804\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0697 - val_loss: 0.0843\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.0668 - val_loss: 0.0766\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 0.0640 - val_loss: 0.0732\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0664 - val_loss: 0.0813\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.0618 - val_loss: 0.0702\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.0599 - val_loss: 0.0781\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0573 - val_loss: 0.0855\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0557 - val_loss: 0.0656\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0551 - val_loss: 0.0723\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0531 - val_loss: 0.0613\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0514 - val_loss: 0.0903\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.0517 - val_loss: 0.0683\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.0505 - val_loss: 0.0667\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0479 - val_loss: 0.0623\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0467 - val_loss: 0.0702\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0473 - val_loss: 0.0580\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0468 - val_loss: 0.0597\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.0451 - val_loss: 0.0570\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.0449 - val_loss: 0.0617\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.0443 - val_loss: 0.0791\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0433 - val_loss: 0.0733\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0422 - val_loss: 0.0763\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0418 - val_loss: 0.0553\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0414 - val_loss: 0.0555\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.0404 - val_loss: 0.0637\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.0404 - val_loss: 0.0517\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0393 - val_loss: 0.0579\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0380 - val_loss: 0.0655\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0387 - val_loss: 0.0590\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.0377 - val_loss: 0.0556\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.0363 - val_loss: 0.0566\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.0365 - val_loss: 0.0798\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.0371 - val_loss: 0.0604\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0361 - val_loss: 0.0808\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0359 - val_loss: 0.0526\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.0355 - val_loss: 0.0514\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.0345 - val_loss: 0.0585\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0344 - val_loss: 0.0556\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0336 - val_loss: 0.0538\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0345 - val_loss: 0.0481\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.0344 - val_loss: 0.0538\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.0343 - val_loss: 0.0911\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0328 - val_loss: 0.0506\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.0325 - val_loss: 0.0488\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.0334 - val_loss: 0.0563\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 255us/step - loss: 0.0314 - val_loss: 0.0641\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0322 - val_loss: 0.0476\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0309 - val_loss: 0.0481\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.0307 - val_loss: 0.0468\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0312 - val_loss: 0.0562\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.0301 - val_loss: 0.0450\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.0290 - val_loss: 0.0532\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.0299 - val_loss: 0.0487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0303 - val_loss: 0.0441\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0290 - val_loss: 0.0471\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.0290 - val_loss: 0.0435\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0289 - val_loss: 0.0471\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.0286 - val_loss: 0.0445\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.0279 - val_loss: 0.0452\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0283 - val_loss: 0.0441\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.0281 - val_loss: 0.0485\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0282 - val_loss: 0.0468\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0274 - val_loss: 0.0455\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0280 - val_loss: 0.0469\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0270 - val_loss: 0.0663\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.0274 - val_loss: 0.0432\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.0269 - val_loss: 0.0433\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0269 - val_loss: 0.0427\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.0268 - val_loss: 0.0478\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0265 - val_loss: 0.0432\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0261 - val_loss: 0.0455\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.0269 - val_loss: 0.0462\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.0266 - val_loss: 0.0529\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0259 - val_loss: 0.0582\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0260 - val_loss: 0.0423\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0250 - val_loss: 0.0408\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0259 - val_loss: 0.0475\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.0254 - val_loss: 0.0449\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.0255 - val_loss: 0.0416\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.0246 - val_loss: 0.0421\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0241 - val_loss: 0.0516\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0247 - val_loss: 0.0405\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0248 - val_loss: 0.0476\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0242 - val_loss: 0.0407\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.0247 - val_loss: 0.0490\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 0.0242 - val_loss: 0.0403\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0233 - val_loss: 0.0441\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 237us/step - loss: 0.0234 - val_loss: 0.0449\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0243 - val_loss: 0.0410\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0232 - val_loss: 0.0411\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0235 - val_loss: 0.0483\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.0226 - val_loss: 0.0404\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.0223 - val_loss: 0.0413\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0224 - val_loss: 0.0401\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 241us/step - loss: 0.0239 - val_loss: 0.0447\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0223 - val_loss: 0.0409\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0221 - val_loss: 0.0459\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0222 - val_loss: 0.0478\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.0224 - val_loss: 0.0405\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0221 - val_loss: 0.0417\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0217 - val_loss: 0.0387\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0215 - val_loss: 0.0409\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0214 - val_loss: 0.0384\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0211 - val_loss: 0.0388\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0213 - val_loss: 0.0500\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 3s 274us/step - loss: 0.0214 - val_loss: 0.0432\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.0213 - val_loss: 0.0446\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0216 - val_loss: 0.0455\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0212 - val_loss: 0.0381\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0214 - val_loss: 0.0409\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0207 - val_loss: 0.0417\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 230us/step - loss: 0.0214 - val_loss: 0.0403\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.0206 - val_loss: 0.0407\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.0209 - val_loss: 0.0461\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0199 - val_loss: 0.0466\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0195 - val_loss: 0.0431\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0200 - val_loss: 0.0388\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0202 - val_loss: 0.0377\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0204 - val_loss: 0.0403\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.0196 - val_loss: 0.0431\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.0197 - val_loss: 0.0493\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0199 - val_loss: 0.0396\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0196 - val_loss: 0.0520\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.0200 - val_loss: 0.0408\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.0198 - val_loss: 0.0390\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0195 - val_loss: 0.0432\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.0193 - val_loss: 0.0402\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.0198 - val_loss: 0.0367\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0190 - val_loss: 0.0437\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0190 - val_loss: 0.0474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0194 - val_loss: 0.0394\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0193 - val_loss: 0.0376\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 231us/step - loss: 0.0194 - val_loss: 0.0415\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0192 - val_loss: 0.0409\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0187 - val_loss: 0.0428\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.0201 - val_loss: 0.0437\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0186 - val_loss: 0.0361\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0193 - val_loss: 0.0411\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0187 - val_loss: 0.0452\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0196 - val_loss: 0.0381\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.0184 - val_loss: 0.0393\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0184 - val_loss: 0.0438\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0188 - val_loss: 0.0366\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0181 - val_loss: 0.0389\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0177 - val_loss: 0.0407\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0181 - val_loss: 0.0382\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0180 - val_loss: 0.0373\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 0.0184 - val_loss: 0.0388\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0179 - val_loss: 0.0358\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0177 - val_loss: 0.0471\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0173 - val_loss: 0.0366\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0175 - val_loss: 0.0355\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0172 - val_loss: 0.0503\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0178 - val_loss: 0.0555\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0181 - val_loss: 0.0412\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0175 - val_loss: 0.0372\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0175 - val_loss: 0.0370\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0174 - val_loss: 0.0381\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0173 - val_loss: 0.0382\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0174 - val_loss: 0.0397\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0170 - val_loss: 0.0405\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0171 - val_loss: 0.0357\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.0169 - val_loss: 0.0380\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 234us/step - loss: 0.0171 - val_loss: 0.0358\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0165 - val_loss: 0.0402\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0169 - val_loss: 0.0357\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0166 - val_loss: 0.0455\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0168 - val_loss: 0.0367\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.0168 - val_loss: 0.0348\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.0169 - val_loss: 0.0343\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.0170 - val_loss: 0.0351\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0164 - val_loss: 0.0354\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0164 - val_loss: 0.0385\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0166 - val_loss: 0.0359\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0163 - val_loss: 0.0392\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0161 - val_loss: 0.0364\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.0164 - val_loss: 0.0364\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0161 - val_loss: 0.0400\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0159 - val_loss: 0.0420\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0164 - val_loss: 0.0450\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0163 - val_loss: 0.0438\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0159 - val_loss: 0.0365\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 235us/step - loss: 0.0158 - val_loss: 0.0342\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 256us/step - loss: 0.0160 - val_loss: 0.0391\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0159 - val_loss: 0.0358\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0157 - val_loss: 0.0349\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0158 - val_loss: 0.0358\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0158 - val_loss: 0.0352\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0157 - val_loss: 0.0371\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 233us/step - loss: 0.0158 - val_loss: 0.0346\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.0157 - val_loss: 0.0353\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 255us/step - loss: 0.0159 - val_loss: 0.0486\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0155 - val_loss: 0.0347\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0157 - val_loss: 0.0349\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.0154 - val_loss: 0.0348\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0153 - val_loss: 0.0373\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0154 - val_loss: 0.0351\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.0152 - val_loss: 0.0341\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.0150 - val_loss: 0.0359\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0154 - val_loss: 0.0353\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0154 - val_loss: 0.0402\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 223us/step - loss: 0.0152 - val_loss: 0.0377\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0153 - val_loss: 0.0352\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.0148 - val_loss: 0.0371\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.0146 - val_loss: 0.0342\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 255us/step - loss: 0.0150 - val_loss: 0.0349\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0151 - val_loss: 0.0398\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0150 - val_loss: 0.0390\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0147 - val_loss: 0.0354\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0146 - val_loss: 0.0353\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0150 - val_loss: 0.0401\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0144 - val_loss: 0.0377\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 0.0149 - val_loss: 0.0378\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0145 - val_loss: 0.0427\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0147 - val_loss: 0.0354\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.0145 - val_loss: 0.0357\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.0149 - val_loss: 0.0342\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0144 - val_loss: 0.0359\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.0146 - val_loss: 0.0363\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.0142 - val_loss: 0.0450\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0142 - val_loss: 0.0383\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0141 - val_loss: 0.0461\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0142 - val_loss: 0.0341\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0141 - val_loss: 0.0387\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0142 - val_loss: 0.0340\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0143 - val_loss: 0.0364\n"
     ]
    }
   ],
   "source": [
    "#se entrena el modelo\n",
    "history = model.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2437/2437 [==============================] - 0s 83us/step\n",
      "\n",
      " Loss: 0.028 \t:\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_scaled, y_test_scaled)\n",
    "print(\"\\n Loss: %.3f \\t:\" %(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se obtienen los gradietes luego de entrenar\n",
    "loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "listOfVariableTensors = model.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Capa 2')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#histograma luego de entrenado\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(141)\n",
    "plt.hist(evaluated_gradients[0])\n",
    "plt.title(\"Capa 1\")\n",
    "plt.subplot(142)\n",
    "plt.hist(evaluated_gradients[1])\n",
    "plt.title(\"Capa 1\")\n",
    "plt.subplot(143)\n",
    "plt.hist(evaluated_gradients[2])\n",
    "plt.title(\"Capa 2\")\n",
    "plt.subplot(144)\n",
    "plt.hist(evaluated_gradients[3])\n",
    "plt.title(\"Capa 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c)\n",
    "modelC = Sequential()\n",
    "modelC.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelC.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelC.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelC.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelC.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelC.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelC.compile(optimizer=sgd,loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtener gradientes sin entrenar red\n",
    "loss = keras.losses.mean_squared_error(modelC.output,y_train_scaled)\n",
    "listOfVariableTensors = modelC.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={modelC.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Capa 4')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Histograma\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(141)\n",
    "plt.hist(evaluated_gradients[0])\n",
    "plt.title(\"Capa 1\")\n",
    "plt.subplot(142)\n",
    "plt.hist(evaluated_gradients[2])\n",
    "plt.title(\"Capa 2\")\n",
    "plt.subplot(143)\n",
    "plt.hist(evaluated_gradients[4])\n",
    "plt.title(\"Capa 3\")\n",
    "plt.subplot(144)\n",
    "plt.hist(evaluated_gradients[6])\n",
    "plt.title(\"Capa 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: 14.6389 - val_loss: 13.7932\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 4s 393us/step - loss: 14.0441 - val_loss: 16.3782\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 14.0012 - val_loss: 15.0458\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 4s 427us/step - loss: 13.9754 - val_loss: 13.9849\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 4s 380us/step - loss: 11.3529 - val_loss: 2.7745\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 4s 377us/step - loss: 1.8270 - val_loss: 0.8914\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: 0.8552 - val_loss: 0.7422\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: 0.6907 - val_loss: 0.6171\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 0.6766 - val_loss: 0.8503\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.6477 - val_loss: 0.4041\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: 0.5520 - val_loss: 0.7391\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 4s 391us/step - loss: 0.5469 - val_loss: 0.3262\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 4s 406us/step - loss: 0.4764 - val_loss: 0.3423\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: 0.4202 - val_loss: 0.9288\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 0.4213 - val_loss: 0.2813\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 0.3602 - val_loss: 0.2895\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.3332 - val_loss: 1.5239\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 0.3131 - val_loss: 0.2079\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 4s 375us/step - loss: 0.3244 - val_loss: 0.2130\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: 0.2908 - val_loss: 0.2070\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: 0.2256 - val_loss: 0.6667\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 4s 376us/step - loss: 0.2059 - val_loss: 0.3746\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.2158 - val_loss: 0.1853\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 0.2108 - val_loss: 0.1328\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.1643 - val_loss: 0.1404\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 4s 387us/step - loss: 0.1621 - val_loss: 0.1172\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 4s 388us/step - loss: 0.1476 - val_loss: 0.1777\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.1512 - val_loss: 0.1173\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.1468 - val_loss: 0.1053\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 3s 359us/step - loss: 0.1474 - val_loss: 0.1081\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 4s 379us/step - loss: 0.1331 - val_loss: 0.1096\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 0.1308 - val_loss: 0.0946\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 0.1038 - val_loss: 0.2869\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 0.1037 - val_loss: 0.0905\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 4s 377us/step - loss: 0.0875 - val_loss: 0.1462\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 4s 381us/step - loss: 0.1316 - val_loss: 0.0884\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.1004 - val_loss: 0.2232\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.1162 - val_loss: 0.0815\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0858 - val_loss: 0.1113\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 4s 382us/step - loss: 0.1097 - val_loss: 0.0829\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 0.0838 - val_loss: 0.1425\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 0.0897 - val_loss: 0.0774\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.0954 - val_loss: 0.0924\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: 0.0919 - val_loss: 0.1353\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 4s 381us/step - loss: 0.0744 - val_loss: 0.0863\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: 0.0812 - val_loss: 0.0705\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 0.0936 - val_loss: 0.0775\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: 0.0842 - val_loss: 0.0679\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 4s 385us/step - loss: 0.0708 - val_loss: 0.0796\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 4s 379us/step - loss: 0.0789 - val_loss: 0.0673\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.0754 - val_loss: 0.0654\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 0.0772 - val_loss: 0.0890\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: 0.0650 - val_loss: 0.0709\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 4s 382us/step - loss: 0.0934 - val_loss: 0.0920\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 0.0587 - val_loss: 0.1394\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0962 - val_loss: 0.0740\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0646 - val_loss: 0.0610\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 4s 399us/step - loss: 0.0749 - val_loss: 0.0817\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 4s 385us/step - loss: 0.0632 - val_loss: 0.0613\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0664 - val_loss: 0.0860\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 0.0634 - val_loss: 0.0595\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: 0.0546 - val_loss: 0.0632\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 4s 374us/step - loss: 0.0776 - val_loss: 0.1424\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 0.0637 - val_loss: 0.0759\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0592 - val_loss: 0.0605\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0533 - val_loss: 0.0558\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: 0.0569 - val_loss: 0.0558\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 4s 372us/step - loss: 0.0579 - val_loss: 0.0578\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0582 - val_loss: 0.0905\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 0.0629 - val_loss: 0.0546\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.0586 - val_loss: 0.0553\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 4s 374us/step - loss: 0.0480 - val_loss: 0.0533\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: 0.0472 - val_loss: 0.0747\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0513 - val_loss: 0.0678\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0618 - val_loss: 0.0515\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.0527 - val_loss: 0.0583\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 4s 372us/step - loss: 0.0461 - val_loss: 0.0496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.0577 - val_loss: 0.0707\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0492 - val_loss: 0.0522\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0534 - val_loss: 0.0530\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0490 - val_loss: 0.0674\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0620 - val_loss: 0.0684\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 0.0468 - val_loss: 0.1006\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0478 - val_loss: 0.0492\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0461 - val_loss: 0.0518\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 4s 359us/step - loss: 0.0495 - val_loss: 0.1097\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 0.0520 - val_loss: 0.0482\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: 0.0502 - val_loss: 0.0525\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0483 - val_loss: 0.1653\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0501 - val_loss: 0.0517\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: 0.0415 - val_loss: 0.0572\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 0.0417 - val_loss: 0.0479\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0452 - val_loss: 0.0515\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0470 - val_loss: 0.0969\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0462 - val_loss: 0.0456\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.0430 - val_loss: 0.0575\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 0.0470 - val_loss: 0.1600\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0451 - val_loss: 0.0514\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0401 - val_loss: 0.0588\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.0412 - val_loss: 0.0465\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 0.0461 - val_loss: 0.0457\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0375 - val_loss: 0.0952\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0507 - val_loss: 0.0575\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0430 - val_loss: 0.0616\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0431 - val_loss: 0.0689\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 0.0406 - val_loss: 0.0488\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.0428 - val_loss: 0.0454\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0402 - val_loss: 0.0513\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0416 - val_loss: 0.0445\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 4s 359us/step - loss: 0.0483 - val_loss: 0.0791\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: 0.0412 - val_loss: 0.0821\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 0.0394 - val_loss: 0.1054\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0361 - val_loss: 0.0550\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0387 - val_loss: 0.0459\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 4s 387us/step - loss: 0.0398 - val_loss: 0.0643\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 4s 393us/step - loss: 0.0382 - val_loss: 0.0444\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0375 - val_loss: 0.0505\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0371 - val_loss: 0.0488\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0384 - val_loss: 0.0589\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.0400 - val_loss: 0.0418\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0363 - val_loss: 0.1033\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0370 - val_loss: 0.0432\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0352 - val_loss: 0.0485\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 359us/step - loss: 0.0346 - val_loss: 0.0534\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.0408 - val_loss: 0.0415\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0330 - val_loss: 0.0415\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0344 - val_loss: 0.0457\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0376 - val_loss: 0.0538\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.0357 - val_loss: 0.0554\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 4s 380us/step - loss: 0.0366 - val_loss: 0.0711\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0349 - val_loss: 0.0470\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0322 - val_loss: 0.0839\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.0333 - val_loss: 0.0914\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 0.0396 - val_loss: 0.0488\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0342 - val_loss: 0.0394\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0309 - val_loss: 0.0892\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0381 - val_loss: 0.1327\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 0.0347 - val_loss: 0.0508\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.0353 - val_loss: 0.0416\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0352 - val_loss: 0.0486\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0355 - val_loss: 0.0398\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.0334 - val_loss: 0.0850\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: 0.0345 - val_loss: 0.0400\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0333 - val_loss: 0.0623\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.0314 - val_loss: 0.0391\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0352 - val_loss: 0.0567\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0296 - val_loss: 0.0391\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 0.0287 - val_loss: 0.0447\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: 0.0338 - val_loss: 0.0388\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0359 - val_loss: 0.0498\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0289 - val_loss: 0.0406\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.0336 - val_loss: 0.0386\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0295 - val_loss: 0.0480\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.0338 - val_loss: 0.0446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0308 - val_loss: 0.0392\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0318 - val_loss: 0.1187\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: 0.0315 - val_loss: 0.0476\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0290 - val_loss: 0.0398\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 0.0288 - val_loss: 0.0428\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0298 - val_loss: 0.0369\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0321 - val_loss: 0.0475\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.0339 - val_loss: 0.0382\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: 0.0289 - val_loss: 0.0465\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0303 - val_loss: 0.0476\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0350 - val_loss: 0.0384\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0319 - val_loss: 0.0629\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 0.0282 - val_loss: 0.0470\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: 0.0272 - val_loss: 0.0449\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0331 - val_loss: 0.0404\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0265 - val_loss: 0.0402\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0279 - val_loss: 0.0395\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: 0.0265 - val_loss: 0.0593\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0241 - val_loss: 0.0394\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0347 - val_loss: 0.0480\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0265 - val_loss: 0.0416\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0287 - val_loss: 0.0369\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 0.0287 - val_loss: 0.0462\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0283 - val_loss: 0.0424\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0273 - val_loss: 0.0554\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0283 - val_loss: 0.0484\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: 0.0272 - val_loss: 0.0356\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0261 - val_loss: 0.0397\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0290 - val_loss: 0.0378\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: 0.0301 - val_loss: 0.0349\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0295 - val_loss: 0.0380\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 0.0283 - val_loss: 0.0391\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: 0.0270 - val_loss: 0.0368\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.0263 - val_loss: 0.0477\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0255 - val_loss: 0.0447\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0238 - val_loss: 0.0471\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 4s 372us/step - loss: 0.0256 - val_loss: 0.0386\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 0.0260 - val_loss: 0.0457\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0292 - val_loss: 0.0342\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0272 - val_loss: 0.0364\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0239 - val_loss: 0.0365\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0246 - val_loss: 0.0551\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 0.0282 - val_loss: 0.0359\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0270 - val_loss: 0.0533\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0283 - val_loss: 0.0447\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0265 - val_loss: 0.0502\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0246 - val_loss: 0.0350\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0263 - val_loss: 0.0392\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0262 - val_loss: 0.0575\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0253 - val_loss: 0.0525\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 0.0242 - val_loss: 0.0351\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.0255 - val_loss: 0.0364\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.0281 - val_loss: 0.0604\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0250 - val_loss: 0.0348\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0250 - val_loss: 0.0374\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 4s 377us/step - loss: 0.0254 - val_loss: 0.0378\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 0.0252 - val_loss: 0.0425\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0239 - val_loss: 0.0345\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0219 - val_loss: 0.0352\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0287 - val_loss: 0.0405\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0224 - val_loss: 0.0345\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 0.0243 - val_loss: 0.0331\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0271 - val_loss: 0.0731\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0244 - val_loss: 0.0355\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0228 - val_loss: 0.0394\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 4s 375us/step - loss: 0.0249 - val_loss: 0.0367\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 0.0233 - val_loss: 0.0338\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0253 - val_loss: 0.0342\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0238 - val_loss: 0.0363\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0223 - val_loss: 0.0345\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.0227 - val_loss: 0.0350\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0235 - val_loss: 0.0431\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0251 - val_loss: 0.0342\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0227 - val_loss: 0.0339\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 0.0233 - val_loss: 0.0349\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0251 - val_loss: 0.0382\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0271 - val_loss: 0.0394\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0223 - val_loss: 0.0590\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.0235 - val_loss: 0.0393\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 0.0266 - val_loss: 0.0358\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.0217 - val_loss: 0.0438\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0238 - val_loss: 0.0364\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0263 - val_loss: 0.0346\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0249 - val_loss: 0.0357\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: 0.0264 - val_loss: 0.0516\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.0222 - val_loss: 0.0368\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0262 - val_loss: 0.0451\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0264 - val_loss: 0.0418\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0244 - val_loss: 0.0341\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: 0.0232 - val_loss: 0.0379\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0237 - val_loss: 0.0323\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0223 - val_loss: 0.0320\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0221 - val_loss: 0.0346\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 0.0222 - val_loss: 0.0332\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0211 - val_loss: 0.0323\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0207 - val_loss: 0.0335\n"
     ]
    }
   ],
   "source": [
    "#Se entrena el modelo\n",
    "history = modelC.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtener gradientes luego de entrenar la red\n",
    "loss = keras.losses.mean_squared_error(modelC.output,y_train_scaled)\n",
    "listOfVariableTensors = modelC.trainable_weights \n",
    "gradientsTrain = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sessTrain = K.get_session()\n",
    "evaluated_gradients = sessTrain.run(gradients,feed_dict={modelC.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Capa 4')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Histograma despues del entrenamiento\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(141)\n",
    "plt.hist(evaluated_gradients[0])\n",
    "plt.title(\"Capa 1\")\n",
    "plt.subplot(142)\n",
    "plt.hist(evaluated_gradients[2])\n",
    "plt.title(\"Capa 2\")\n",
    "plt.subplot(143)\n",
    "plt.hist(evaluated_gradients[4])\n",
    "plt.title(\"Capa 3\")\n",
    "plt.subplot(144)\n",
    "plt.hist(evaluated_gradients[6])\n",
    "plt.title(\"Capa 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D)\n",
    "modelD = Sequential()\n",
    "modelD.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modelD.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modelD.add(Dense(256,  kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modelD.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modelD.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "modelD.add(Dense(1, kernel_initializer='glorot_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelD.compile(optimizer=sgd,loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = keras.losses.mean_squared_error(modelD.output,y_train_scaled)\n",
    "listOfVariableTensors = modelD.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={modelD.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Capa 4')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Histograma\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(141)\n",
    "plt.hist(evaluated_gradients[0])\n",
    "plt.title(\"Capa 1\")\n",
    "plt.subplot(142)\n",
    "plt.hist(evaluated_gradients[2])\n",
    "plt.title(\"Capa 2\")\n",
    "plt.subplot(143)\n",
    "plt.hist(evaluated_gradients[4])\n",
    "plt.title(\"Capa 3\")\n",
    "plt.subplot(144)\n",
    "plt.hist(evaluated_gradients[6])\n",
    "plt.title(\"Capa 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 14.6424 - val_loss: 15.7349\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: 5.1592 - val_loss: 2.7362\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 1.8445 - val_loss: 1.1849\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 1.0882 - val_loss: 0.7623\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.9675 - val_loss: 0.4590\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.7771 - val_loss: 1.2609\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.6399 - val_loss: 0.4548\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.5749 - val_loss: 0.6625\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.5232 - val_loss: 0.5627\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.4763 - val_loss: 0.3498\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.3869 - val_loss: 0.5530\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.3820 - val_loss: 0.2251\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.3522 - val_loss: 0.2538\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.3325 - val_loss: 0.2031\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.2796 - val_loss: 0.2008\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.2485 - val_loss: 0.4093\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 0.2719 - val_loss: 0.1465\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.2581 - val_loss: 0.2162\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.1970 - val_loss: 0.2744\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.1870 - val_loss: 0.2738\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.1942 - val_loss: 0.1585\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.1854 - val_loss: 0.1362\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.1795 - val_loss: 0.4257\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.1919 - val_loss: 0.1311\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.1613 - val_loss: 0.1348\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.1442 - val_loss: 0.5551\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.1398 - val_loss: 0.0967\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.1493 - val_loss: 0.0982\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.1198 - val_loss: 0.3100\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.1146 - val_loss: 0.2370\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.1285 - val_loss: 0.1158\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.1398 - val_loss: 0.0913\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0910 - val_loss: 0.1123\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.1017 - val_loss: 0.0847\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0996 - val_loss: 0.1067\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.1268 - val_loss: 0.1778\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 359us/step - loss: 0.1068 - val_loss: 0.0750\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: 0.0880 - val_loss: 0.0807\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.0936 - val_loss: 0.0732\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0902 - val_loss: 0.0718\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0967 - val_loss: 0.1333\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0936 - val_loss: 0.2192\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 0.0804 - val_loss: 0.0756\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0954 - val_loss: 0.0695\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0897 - val_loss: 0.0801\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0749 - val_loss: 0.0736\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: 0.0888 - val_loss: 0.0656\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 0.0937 - val_loss: 0.1378\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0754 - val_loss: 0.0787\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0821 - val_loss: 0.0825\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0723 - val_loss: 0.0970\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: 0.0690 - val_loss: 0.1066\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 4s 380us/step - loss: 0.0686 - val_loss: 0.0779\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 0.0804 - val_loss: 0.0728\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0748 - val_loss: 0.0668\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.0670 - val_loss: 0.0786\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.0687 - val_loss: 0.1054\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 0.0716 - val_loss: 0.0603\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0652 - val_loss: 0.0581\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0727 - val_loss: 0.0750\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0634 - val_loss: 0.0584\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 3s 357us/step - loss: 0.0680 - val_loss: 0.0592\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.0661 - val_loss: 0.0571\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0528 - val_loss: 0.1662\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0617 - val_loss: 0.0808\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0523 - val_loss: 0.0617\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 3s 357us/step - loss: 0.0621 - val_loss: 0.0728\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: 0.0657 - val_loss: 0.0559\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0473 - val_loss: 0.0990\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0563 - val_loss: 0.0882\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0640 - val_loss: 0.0533\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 3s 359us/step - loss: 0.0697 - val_loss: 0.0694\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0587 - val_loss: 0.1561\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0558 - val_loss: 0.0532\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0545 - val_loss: 0.0556\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 0.0564 - val_loss: 0.0996\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 0.0527 - val_loss: 0.0469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0512 - val_loss: 0.1139\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0489 - val_loss: 0.1060\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0564 - val_loss: 0.0510\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.0601 - val_loss: 0.0483\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 0.0526 - val_loss: 0.0566\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0546 - val_loss: 0.0479\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0496 - val_loss: 0.0761\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0512 - val_loss: 0.0504\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.0483 - val_loss: 0.0570\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.0460 - val_loss: 0.0501\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0475 - val_loss: 0.0492\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0466 - val_loss: 0.0514\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0496 - val_loss: 0.0546\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.0516 - val_loss: 0.0469\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0532 - val_loss: 0.0471\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0414 - val_loss: 0.0797\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0471 - val_loss: 0.0582\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0455 - val_loss: 0.0457\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.0487 - val_loss: 0.2122\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0423 - val_loss: 0.1153\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0411 - val_loss: 0.0867\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0452 - val_loss: 0.0452\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0395 - val_loss: 0.0473\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.0368 - val_loss: 0.0424\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0423 - val_loss: 0.0424\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0476 - val_loss: 0.0466\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0444 - val_loss: 0.1281\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0486 - val_loss: 0.0596\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.0384 - val_loss: 0.0519\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0396 - val_loss: 0.0615\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0487 - val_loss: 0.0890\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0399 - val_loss: 0.0779\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0388 - val_loss: 0.0794\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0348 - val_loss: 0.0474\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 0.0447 - val_loss: 0.0466\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0398 - val_loss: 0.0431\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0434 - val_loss: 0.0413\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0373 - val_loss: 0.1251\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 4s 382us/step - loss: 0.0382 - val_loss: 0.0434\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0410 - val_loss: 0.0410\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0418 - val_loss: 0.0417\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0413 - val_loss: 0.0502\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0369 - val_loss: 0.0534\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 0.0374 - val_loss: 0.0433\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: 0.0369 - val_loss: 0.0407\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0395 - val_loss: 0.0455\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0406 - val_loss: 0.0472\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0427 - val_loss: 0.0454\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0332 - val_loss: 0.0521\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 0.0345 - val_loss: 0.0414\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0380 - val_loss: 0.0515\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0359 - val_loss: 0.0398\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0318 - val_loss: 0.0647\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: 0.0393 - val_loss: 0.0856\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0308 - val_loss: 0.0377\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0372 - val_loss: 0.0390\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0311 - val_loss: 0.0416\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0316 - val_loss: 0.0637\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0341 - val_loss: 0.0529\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0326 - val_loss: 0.0447\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0383 - val_loss: 0.0480\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0333 - val_loss: 0.0436\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0343 - val_loss: 0.0413\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 3s 359us/step - loss: 0.0294 - val_loss: 0.0750\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.0313 - val_loss: 0.0691\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0456 - val_loss: 0.0423\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0311 - val_loss: 0.0375\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 0.0334 - val_loss: 0.0421\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 0.0343 - val_loss: 0.0438\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: 0.0314 - val_loss: 0.0548\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.0384 - val_loss: 0.0423\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 0.0331 - val_loss: 0.0376\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 0.0287 - val_loss: 0.0512\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0322 - val_loss: 0.0441\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0304 - val_loss: 0.0362\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0313 - val_loss: 0.0914\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0329 - val_loss: 0.0957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0336 - val_loss: 0.0375\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 0.0291 - val_loss: 0.0388\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0321 - val_loss: 0.0390\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0280 - val_loss: 0.0348\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0313 - val_loss: 0.0360\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 0.0308 - val_loss: 0.0608\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 0.0322 - val_loss: 0.0686\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0328 - val_loss: 0.0354\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0274 - val_loss: 0.0684\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0307 - val_loss: 0.0387\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 0.0276 - val_loss: 0.0356\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0281 - val_loss: 0.0357\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0297 - val_loss: 0.0349\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0300 - val_loss: 0.0495\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0329 - val_loss: 0.0587\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: 0.0343 - val_loss: 0.0382\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0302 - val_loss: 0.0549\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0304 - val_loss: 0.0358\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0260 - val_loss: 0.0357\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0273 - val_loss: 0.0484\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: 0.0297 - val_loss: 0.0341\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0282 - val_loss: 0.0458\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0276 - val_loss: 0.0619\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0333 - val_loss: 0.0403\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0241 - val_loss: 0.0436\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.0241 - val_loss: 0.0496\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0287 - val_loss: 0.0456\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0266 - val_loss: 0.0336\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0266 - val_loss: 0.0389\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0274 - val_loss: 0.0364\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.0268 - val_loss: 0.0383\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0250 - val_loss: 0.0449\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0255 - val_loss: 0.0519\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0265 - val_loss: 0.0410\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0280 - val_loss: 0.0352\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: 0.0283 - val_loss: 0.0363\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 0.0260 - val_loss: 0.0503\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0246 - val_loss: 0.0352\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0276 - val_loss: 0.0657\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0280 - val_loss: 0.0328\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0252 - val_loss: 0.0566\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 0.0281 - val_loss: 0.0397\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0310 - val_loss: 0.0477\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0245 - val_loss: 0.0360\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0276 - val_loss: 0.0326\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 0.0221 - val_loss: 0.0372\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 0.0258 - val_loss: 0.0354\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0236 - val_loss: 0.0430\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0240 - val_loss: 0.0337\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0246 - val_loss: 0.0631\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0250 - val_loss: 0.0459\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.0246 - val_loss: 0.0620\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0256 - val_loss: 0.0402\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0264 - val_loss: 0.0457\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0244 - val_loss: 0.0379\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0298 - val_loss: 0.0775\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 4s 376us/step - loss: 0.0278 - val_loss: 0.0331\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0260 - val_loss: 0.0337\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0279 - val_loss: 0.0328\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 0.0246 - val_loss: 0.0866\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 4s 359us/step - loss: 0.0246 - val_loss: 0.0475\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.0234 - val_loss: 0.0576\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0240 - val_loss: 0.0362\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0258 - val_loss: 0.0627\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 0.0242 - val_loss: 0.0569\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 3s 359us/step - loss: 0.0244 - val_loss: 0.0405\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.0256 - val_loss: 0.0698\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0235 - val_loss: 0.0375\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0254 - val_loss: 0.0332\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: 0.0231 - val_loss: 0.0339\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0239 - val_loss: 0.0344\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 0.0234 - val_loss: 0.0335\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0228 - val_loss: 0.0439\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0241 - val_loss: 0.0515\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.0221 - val_loss: 0.0406\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.0245 - val_loss: 0.0408\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0233 - val_loss: 0.0332\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0237 - val_loss: 0.0315\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0235 - val_loss: 0.0362\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 0.0230 - val_loss: 0.0541\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.0248 - val_loss: 0.0439\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0233 - val_loss: 0.0330\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0209 - val_loss: 0.0327\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0242 - val_loss: 0.0472\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 0.0244 - val_loss: 0.0425\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: 0.0216 - val_loss: 0.0332\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.0229 - val_loss: 0.0352\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0250 - val_loss: 0.0370\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0225 - val_loss: 0.0308\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.0210 - val_loss: 0.0310\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0224 - val_loss: 0.0567\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0226 - val_loss: 0.0363\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0223 - val_loss: 0.0328\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0217 - val_loss: 0.0459\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.0232 - val_loss: 0.0332\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0233 - val_loss: 0.0557\n"
     ]
    }
   ],
   "source": [
    "history = modelD.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtener gradientes luego de entrenar la red\n",
    "loss = keras.losses.mean_squared_error(modelD.output,y_train_scaled)\n",
    "listOfVariableTensors = modelD.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={modelD.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Capa 4')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Histograma despues de entrenar\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(141)\n",
    "plt.hist(evaluated_gradients[0])\n",
    "plt.title(\"Capa 1\")\n",
    "plt.subplot(142)\n",
    "plt.hist(evaluated_gradients[2])\n",
    "plt.title(\"Capa 2\")\n",
    "plt.subplot(143)\n",
    "plt.hist(evaluated_gradients[4])\n",
    "plt.title(\"Capa 3\")\n",
    "plt.subplot(144)\n",
    "plt.hist(evaluated_gradients[6])\n",
    "plt.title(\"Capa 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E)\n",
    "modelE = Sequential()\n",
    "modelE.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='relu'))\n",
    "modelE.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "modelE.add(Dense(256,  kernel_initializer='uniform',activation='relu'))\n",
    "modelE.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "modelE.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "modelE.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelE.compile(optimizer=sgd,loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtener gradientes luego de entrenar la red\n",
    "loss = keras.losses.mean_squared_error(modelE.output,y_train_scaled)\n",
    "listOfVariableTensors = modelE.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={modelE.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.,   3.,   2.,  12.,  24., 116.,  62.,  16.,  14.,   5.]),\n",
       " array([-0.4824062 , -0.39956605, -0.3167259 , -0.23388575, -0.15104559,\n",
       "        -0.06820545,  0.0146347 ,  0.09747486,  0.180315  ,  0.26315516,\n",
       "         0.3459953 ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAHG5JREFUeJzt3X+QHOV95/H3J8Iih41BisQPA2IEpUPSkTOYPUGKcwLIICDkpLozh6gEZB8uBQMuu2KuvI5z5S1f/pCpw1WmTJlTYmJxZYxwMEZlhLEsiTOuCphdIn6IRZFQsFmkIAkIhsJnH5fv/THP4mGZ3Z2d6enumf68qqam++lnpr+z+8x+t59+nm5FBGZmVk2/VXQAZmZWHCcBM7MKcxIwM6swJwEzswpzEjAzqzAnATOzCnMSMDOrMCcBM7MKcxIwM6uww4oOYCrz5s2LWq1WdBjWx0ZGRg5FxPy89+u2bd00k3Zd6iRQq9UYHh4uOgzrY5J+VsR+3batm2bSrt0dZGZWYaU+EjCoDd7P8+v+cMo6t167jetvuyCniMzaVxu8v63XTfcdsPb5SMDMrMKcBMzMKsxJoAeMLl5SdAhm1qecBMzMKsxJwMyswqYdHSTpduAy4EBEnJ7K5gIbgRrwPPCfI+JVSQK+ClwKvAl8LCIeT69ZA/xFetu/jIgN2X4UM+tXHlXUPa0cCXwTuHhC2SCwNSIWAVvTOsAlwKL0WAt8Hd5OGl8EzgaWAV+UNKfT4M3MrDPTJoGI+DHwyoTilcD4f/IbgFUN5XdE3SPA0ZKOB1YAWyLilYh4FdjCuxOLmZnlrN1zAsdGxH6A9HxMKj8BeKGh3lgqm6z8XSStlTQsafjgwYNthlc+HuHTHyQ9L+kpSTskDaeyuZK2SNqdnn2Uaz0j6xPDalIWU5S/uzBifUQMRMTA/Pm5X9crd8dt31F0CDZz50fEGRExkNYn6x41K712k8BLqZuH9HwglY8BJzXUOxHYN0W5WT+YrHvUrPTaTQKbgDVpeQ1wX0P51ao7B3gtdRc9CFwkaU46VL4olVXCtCMbho7KJxDLQgA/lDQiaW0qm6x79B36tavTelsrQ0S/DZwHzJM0Rn2UzzrgbknXAD8HLk/VN1MfHrqH+hDRjwNExCuS/jvwWKr3pYiYeLLZrBecGxH7JB0DbJH0bKsvjIj1wHqAgYGBpt2hZnmbNglExJWTbFrepG4A10/yPrcDt88oOrOSiYh96fmApHupD3l+SdLxEbF/QveoWel5xnAP2brtVHcdFUjSeyUdOb5MvVvzaSbvHjUrPd9PwKx1xwL31ifGcxhwZ0T8QNJjNO8eNSs9JwGzFkXEXuCDTcpfpkn3qFkvcHdQBY0NPlx0CGZWEk4CZmYV5iRgZlZhTgJF8AgfMysJnxg2s77l+xBMz0nAzGas3T+uVj7uDjIzqzAnATOzCnMS6HU+yWxmHXAS6FGe8GVmWXASMDOrMCcBM7MK8xBRM7MJqjS/wEcCJTI0NFR0CGZWMU4CJXHrtdu6+v5bt506o/qeDGRWDU4CZmYV5nMCXXDc9h380/lnFB2GWUt81FdtPhKYoZuvuKzoEMzMMuMk0COcfMysG5wE+tzo4iVFh2BmJeZzAhnI+xzA6OIlHLlqfW77M7PWtHN+pei5BZU8EuiVrhXPGzCzbqtkEjAzs7rcu4MkXQx8FZgF/HVErMs7hiyNDT4MK44sOgwrWBnatYd6WjtyTQKSZgG3AhcCY8BjkjZFxDN5xmGWJbdr60TR1ynKuztoGbAnIvZGxK+Bu4CVOcdgljW3a+tZeXcHnQC80LA+BpydcwyF2rrtVJZTH+Gz5NnRosOxbGTart2tY3lSROS3M+lyYEVEfCKtXwUsi4hPNdRZC6xNq6cBuzLY9TzgUAbvkyfHnI/TIqKjkzqttOtU3o22PZle/F1Mx5+pdSdHxPxWKuZ9JDAGnNSwfiKwr7FCRKwHMh0EL2k4IgayfM9uc8z5kDScwdtM266hO217Mr34u5iOP1N35H1O4DFgkaSFkmYDq4FNOcdgljW3a+tZuR4JRMRbkm4AHqQ+lO72iNiZZwxmWXO7tl6W+zyBiNgMbM55t714jQXHnI9MYi6oXU+lF38X0/Fn6oJcTwybmVm5+LIRZmYV1tNJQNJcSVsk7U7PcyaptybV2S1pTUP5WZKekrRH0i2SlMqHJL0oaUd6XJpBrBdL2pX2Ndhk++GSNqbtj0qqNWz7fCrfJWlFq+9ZwnifTz/vHRmNyskkZkm/I2m7pDckfW3Ca5q2kaL0Uptv4bP01HdiOr34nQEgInr2AdwEDKblQeDLTerMBfam5zlpeU7a9lPg9wABDwCXpPIh4MYM45wFPAecAswGngCWTqhzHXBbWl4NbEzLS1P9w4GF6X1mtfKeZYo3bXsemNelttBJzO8F/j1wLfC1Ca9p2kbc5svXxrr5nSji86RtXfvOjD9KfU5g3rx5UavVig7D+tjIyMihSJNqJH0MGIiIG9L68cD2iFic1q8EzouIP+10v27b1k2N7Xo6pb6pTK1WY3i4O0dAZgCSXpxi8wnUJ4KNG0tlHXPbtm6S9LNW65Y6CZjlYKpD4Wb9/+U9dO4BRV8x096tp08MV0Ft8P6m9wkeG3y4gGj60v+bYtsY9UtAjGt6OQizXuYkYDaJiNgPvC7pnDSK5mrgvoLDMsuUu4PMqA/FA94PzJa0Crgo6jeF+STwTeBfUR9N80BRMZp1g5NADxpdvIQjVxU+27yvRERtkvJh4PR8ozHLj7uDzMwqzEnAzKzC2k4Ckk5KU+1HJe2U9OlU3nRau+puSVOjn5T0oaw+hJmZtaeTI4G3gM9GxBLgHOB6SUupT2XfGhGLgK1pHeASYFF6rAW+3sG+zcwsA20ngYjYHxGPp+XXgVHqsylXAhtStQ3AqrS8Ergj6h4Bjk7T8m3oqKIjMLOKyuScQLoa3pnAo8CxaXz1+DjrY1K1E4AXGl7WdAq+pLWShiUNHzx4MIvwSqHZhC8zs6J1PERU0vuAe4DPRMQvprjSbktT8KPhZtwDAwOeom9mvtxEF3V0JCDpPdQTwLci4rup+KXxbp70fCCVjwEnNbzcU/DNzArWyeggAd8ARiPiKw2bNgHjN7FYw2+m2W8Crk6jhM4BXhvvNjIzs2J0ciRwLnAVcMGEuxGtAy6UtBu4MK1D/Sbce4E9wF9Rv8GCJbdeu63oEKwFze701OrdvszKqO1zAhHxE5r38wMsb1I/gOvb3Z9ZiZwfEYca1seHRa9LtxUcBD5XTGhmM+MZw2adm2xYtFnpOQmYzUwAP5Q0ImltKptsWLRZ6fkqomYzc25E7JN0DLBF0rOtvjAljbUACxYs6FZ8ZjPiIwGzGYiIfen5AHAvsIzJh0VPfO36iBiIiIH581u6B7hZ1zkJ9LqhozyyKCeS3ivpyPFl4CLgaSYfFm1Weu4OMmvdscC9aVb8YcCdEfEDSY8Bd0u6Bvg5cHmBMZrNiJOAWYsiYi/wwSblL9NkWLRZL3B3UA7ave6JmVm3OQmYmVWYk4CZWYX5nEBBRhcvYcmzo0WHYdbXfAnq6TkJmNmM+TxX/3B3kJlZhTkJmJlVmJNAgcYGHy46BDOrOCcBM7MKcxLI0c1XXFZ0CGZm7+AkYGZWYU4CZmYV5iTQQ7ZuO7XoEMyszzgJ5CzLP+THbd+R2XuZWTU5CZiZVZiTQIkMDQ1Nuu6RRWbWDb52kFmf8PV8rB1OAr1m6CjgAzN+2dZtp7L8gueyj8esD1Xp6qNOAmZmGWkneRSdOHI/JyDpYkm7JO2RNJj3/rNQ9qGaPn+Qv35o11ZNuSYBSbOAW4FLgKXAlZKW5hlDKyaeoM1+B0d19/0tV73Srs2aybs7aBmwJyL2Aki6C1gJPJNzHJXhO5jlItN27RO81VL0+QdFRCZv1NLOpI8CF0fEJ9L6VcDZEXFDQ521wNq0ehqwq8thzQMOdXkf7ShjXGWMCTqL6+SImN/Jzltp16m8W227rL+XrPT754PsP2PL7TrvIwE1KXtHFoqI9cD6fMIBScMRMZDX/lpVxrjKGBOUIq5p2zV0r22X4PN3Vb9/Pij2M+Z9YngMOKlh/URgX84xmGXN7dp6Vt5J4DFgkaSFkmYDq4FNOcdgljW3a+tZuXYHRcRbkm4AHgRmAbdHxM48Y2git66nGSpjXGWMCQqOqwTtuqy/l6z0++eDAj9jrieGzcysXHwBOTOzCnMSMDOrsEokAUlzJW2RtDs9z5mk3ppUZ7ekNansCEn3S3pW0k5J6zqMZcrLC0g6XNLGtP1RSbWGbZ9P5bskregkjqziknShpBFJT6XnC8oQV8P2BZLekHRjlnHlqUztN2tl/T5kpazfq3eIiL5/ADcBg2l5EPhykzpzgb3peU5angMcAZyf6swGHgYuaTOOWcBzwCnpvZ4Alk6ocx1wW1peDWxMy0tT/cOBhel9ZmX08+kkrjOBD6Tl04EXM/y9tR1Xw/Z7gO8AN85gv7cDB4CnJ7SPLcDu9DwnlQu4BdgDPAl8qF/bbxc+Vym/DyX5fF37Xk18lPrE8Lx586JWqxUdhvWxkZGRQzFhZqWk3wfeAO6IiNNT2U3AKxGxLv1HNyciPifpUuBTwKXA2cBXI+Ls6fbrtm3d1KxdT6bUl5Ku1WoMDw8XHYb1MUk/m1gWET+e2K1E/VpA56XlDcBDwOdS+R1R/2/qEUlHSzo+IvZPtV+3beumZu16MqVOAla/uNQD37uRJc+OvuPGMGODD3Piug8XHF2lHDv+hz0i9ks6JpWfALzQUG8slU2ZBGxmir7IWj+rxInhfjO6eEnRIdhvtHTdIKhfQE7SsKThgwcPdjkss9Y4CZi15iVJxwOk5wOpvOXrBkXE+ogYiIiB+fM7unCpWWacBPrArdduKzqEKtgErEnLa4D7GsqvVt05wGvTnQ8wKxMngV7nu5RlTtK3gb8DTpM0JukaYB1woaTdwIVpHWAz9eGYe4C/oj7kz6xn+MSw2QQRceUkm5Y3qRvA9d2NyKx72j4SkHSSpO2SRtNMxE+n8qazG9Ph8i1pZtyTkj6U1YcwM7P2dNId9Bbw2YhYApwDXK/6zbUHga0RsQjYmtahfhPuRemxFvh6B/s2M7MMtJ0EImJ/RDyell8HRqmPj15JfTIN6XlVWn57Uk1EPAIcPT7awszMipHJieE0u/JM4FEmTKoBpptUY2ZmBek4CUh6H/WLdH0mIn4xVdUmZe+aVOMJNWZm+ekoCUh6D/UE8K2I+G4q7mhSjSfUmJnlp5PRQQK+AYxGxFcaNnlSTTd5XoCZZaiTeQLnAlcBT0nakcr+nPokmrvTBJufA5enbZupX253D/Am8PEO9m1mZhloOwlExE9o3s8PnlRjZtYTfNkIM7MKcxIwM6swJ4Eedtz2HdNXMjObgi8gZ2a5afcOYdY9PhIwM6swJwEzswpzEiixoaGhokMwsz7nJGBmVmFOAmZmFeYkYGZWYU4CZmYV5iRgZlZhTgIFGV28pOgQzMycBPLiP/pmVkZOAhWxddupRYdgZiXkaweZWd9q91pFz6/7w4wjKS8fCZiZVZiTgJlZhTkJmJlVmJNAj7j5isuKDsHM+pCTQMF8dzAzK5JHB5nZjPkOYf3DRwJmZhXmJGBmVmFOAiXiO4mZWd58TsCs4ty/X225HwlIuljSLkl7JA3mvX+zbnC7tl6V65GApFnArcCFwBjwmKRNEfFMnnFkYugoGHqt6CisBPqqXRuQ79FR0dcpyrs7aBmwJyL2Aki6C1gJ9NSXZeu2U1neyWsveC7TeKxwmbZrX/SsWor+feedBE4AXmhYHwPOzjmGUrr12m1wXNFRZKM2eH/V/iCVol27b9/aoYjIb2fS5cCKiPhEWr8KWBYRn2qosxZYm1ZPA14GDuUWZHPzHEPfxnByRMzv5A1aadepfGLb3tXC2/fjz7xdjuOdpoqj5Xad95HAGHBSw/qJwL7GChGxHlg/vi5pOCIG8gmvOcfgGKYxbbuGd7ftVpTh85YhBsfRvTjyHh30GLBI0kJJs4HVwKacYzDLmtu19axcjwQi4i1JNwAPArOA2yNiZ54xmGXN7dp6We6TxSJiM7B5Bi+Z0eFzlziGOscwiTbadavK8HnLEAM4jokyiSPXE8NmZlYuvnaQmVmFlSIJSJoraYuk3el5ziT11qQ6uyWtabJ9k6Sn845B0hGS7pf0rKSdktbNcN9TXnJA0uGSNqbtj0qqNWz7fCrfJWnFzD515zFIulDSiKSn0vMFee6/YfsCSW9IurGd/RdlBu3uB5L+WdL3J5QvTD+P3ennM7vLcTT9Dkp6KP3+dqTHMTPcf+HfgU7ikFST9MuGz39bF2P4fUmPS3pL0kcnbJvyb2RTEVH4A7gJGEzLg8CXm9SZC+xNz3PS8pyG7f8RuBN4Ou8YgCOA81Od2cDDwCUt7ncW8BxwSnrtE8DSCXWuA25Ly6uBjWl5aap/OLAwvc+sNj57JzGcCXwgLZ8OvJjn/hu23wN8B7ix6PacdbtL25YDfwR8f0L53cDqtHwb8Mm823/a9hAw0Oa+C/8OZBBHjTb/9rQRQw34t8AdwEdb+f1M9SjFkQD1KfYb0vIGYFWTOiuALRHxSkS8CmwBLgaQ9D7gz4C/LCKGiHgzIrYDRMSvgcepjxVvxduXHEivHb/kwGSx/S2wXJJS+V0R8auI+EdgT3q/mWo7hoj4+4gYHxO/E/htSYfntX8ASauoN/heHJHTSrsjIrYCrzeWpc9/AfWfx5SvzyiOSb+DHSrDd6DTOLIybQwR8XxEPAn8y4TXtvX7KcWJYUn/HBFHN6y/GhFz5s2bF7VarcDIrN+NjIwcig5nDLfDbdu6aWRk5BBwC/DLiPgfU9XNbYiopB/R/Oo4X5jsNbVajeHh4e4FZZUn6Wc57ONdbf+ss85y27auaWjX0/6Xn1sSiIiPTLZN0kuSjo+I/ZKOBw7kFVe/Ghoa4pv/59/xwPduZPMHT+WMPx1l+QXPMbp4CUeuWs+J6z5cdIiV0aztDwwMFH8I3kOKvtJmjzqR+rmaKZXlnMAmYPxM9hrgvgJjMTPrdbOAi6jPYp9SWZLAOuBCSbup35hjRkMszbIk6XZJB9Qw3HiyIZSquyUN53tS0oeKi9zsbUuAL0XEK9NVLEUSiIiXI2J5RCxKz9MGbtZF3+TdoyoGga0RsQjYmtYBLgEWpcda4Os5xWg2lacj4m9aqViKJGBWJhHxY2DiPyKTDaFcCdwRdY8AR6fzWmY9wUnArDXHRsR+gPQ8PiO22V3FTsg5NrO2tZ0EJJ0kabukUdUvlfDpVO6+014xdFTREfSDZhOFmo78kbRW0rCk4YMHD3Y5LLPWdHIk8Bbw2YhYApwDXC9pKe47tf700ng3z4RhzC3dVQzqdxaLiIGIGJg/P/f5aWZNtZ0EImJ/RDyell8HRqkfBrvvtGA3X3HZtHWO274jh0j6ymTDmDcBV6cj3XOA18a7jcx6QSaTxdKV9M4EHmVC32nD1QQn6zv1F8ZKRdK3gfOAeZLGgC9SH7Z8t6RrgJ8Dl6fqm4FLqV+z5k3g47kHbNaBjpNAunjbPcBnIuIXU1xLqaW+U0lrqXcXsWDBgk7DM5uxiLhykk3Lm9QN4PruRmTWPR2NDpL0HuoJ4FsR8d1U3FHfqftNu8gngs1sgk5GBwn4BjAaEV9p2OS+UzOzHtFJd9C5wFXAU5LGzzL+Oe47NTPrGW0ngYj4Cc37+cF9p2ZmPcEzhs3MKsxJwMyswpwEzMwqzEnAzKzCnATMzCrMScDe9rsbfrfoEMwsZ7ndaN7MLG++Qf30fCRgZlZhTgJmZhXmJGBmVmFOAmZmFeYkYGZWYU4CZmYV5iRgZlZhTgJmZhXmJGBmVmFOAmZmFeYkYE21O93ezHqLk4CZWYU5CZiZVZivImpmuXE3Y/n4SMDMrMKcBGxaW7edWnQIZtYlTgJmZhXmcwJmZhPkee6i6LuYOQlYx47bvoN/Ov+MosOwHPkEb//IvTtI0sWSdknaI2kw7/33k+O27yg6BEvcrq1X5ZoEJM0CbgUuAZYCV0pammcMZllzu7Zelnd30DJgT0TsBZB0F7ASeCbnOMyy5HZtbWu3ay2rcwl5J4ETgBca1seAs3OOwdo0ungJS54dnbLO0NAQQ0NDM37vm6+4jM9u/H6bkRUu03bt/nbLkyIiv51JlwMrIuITaf0qYFlEfKqhzlpgbVo9DdiVW4CdmQccKjqINvRi3FnGfHJEzO/kDVpp16m82227DL9Lx/AbRcbRcrvO+0hgDDipYf1EYF9jhYhYD6zPM6gsSBqOiIGi45ipXoy7hDFP266h+227DD8Xx1C+OKaT9+igx4BFkhZKmg2sBjblHINZ1tyurWfleiQQEW9JugF4EJgF3B4RO/OMwSxrbtfWy3KfLBYRm4HNee83Bz3XhZX0Ytyli7kk7boMPxfH8BtliWNKuZ4YNjOzcvEF5MzMKsxJYAYkzZW0RdLu9DxnknprUp3dktY0lD+ULi2wIz2O6WKsU17GQNLhkjam7Y9KqjVs+3wq3yVpRbdizDJuSTVJv2z42d6WZ9xFaLU9prrvl/SipK/lHYOkMyT9naSdkp6UdEVG+267jWephTj+TNIz6bNvlXRyN+JoW0T40eIDuAkYTMuDwJeb1JkL7E3Pc9LynLTtIWAghzhnAc8BpwCzgSeApRPqXAfclpZXAxvT8tJU/3BgYXqfWTn9fDuJuwY8XXQbKVt7bKj7VeBO4Gt5xwD8a2BRWv4AsB84uqi2kvHnbyWO84Ej0vInuxFHJw8fCczMSmBDWt4ArGpSZwWwJSJeiYhXgS3AxTnFN+7tyxhExK+B8csYNGr8LH8LLJekVH5XRPwqIv4R2JPer+xxV1Er7RFJZwHHAj8sIoaI+IeI2J2W9wEHgI4m6FGetjJtHBGxPSLeTKuPUJ9HUhpOAjNzbETsB0jPzbpzml1C4ISG9b9J3RX/rYt/vKaL4R11IuIt4DXgd1p8bbd0EjfAQkl/L+l/S/pwt4MtgWnbo6TfAm4G/mtRMUyIZxn1/5if63C/nbaVrMz0+3IN8EDGMXTE9xOYQNKPgOOabPpCq2/RpGx8CNYfR8SLko4E7gGuAu6YeZQdxTBdnVZe2y2dxL0fWBARL6f/fL8n6d9ExC+yDjJPGbTH64DNEfFCu/9zZBDD+PscD/wvYE1E/EtbwTS8XZOyVttKllreh6Q/AQaAP8g4ho44CUwQER+ZbJuklyQdHxH7U4M+0KTaGHBew/qJ1M8FEBEvpufXJd1J/VCyG0mglcsYjNcZk3QYcBTwSouv7Za24456h+uvACJiRNJz1Puih7sedRdl0B5/D/iwpOuA9wGzJb0RES3f8yCDGJD0fuB+4C8i4pFW9z2FTtp4llr6vkj6CPWk+QcR8auMY+iIu4NmZhMwPtpnDXBfkzoPAhdJmpNGSlwEPCjpMEnzACS9B7gMeLpLcbZyGYPGz/JRYFv6Q7oJWJ1GViwEFgE/7VKcmcUtab7q1/VH0ikp7r05xV2UadtjRPxxRCyIiBpwI3DHTBJAFjGk3+W9ad/fyWi/nbTxLE0bh6Qzgf8J/IeIaJokC1X0meleelDvT9wK7E7Pc1P5APDXDfX+C/UTqnuAj6ey9wIjwJPATuqjNbo26ga4FPgH6n2vX0hlX6LeEAF+G/hOivGnwCkNr/1Cet0u4JKcf8ZtxQ38p/RzfQJ4HPijottLWdpjQ/2Pkf3ooGljAP4E+L/AjobHGUW1lQLa7I+Alxo++6ai207jwzOGzcwqzN1BZmYV5iRgZlZhTgJmZhXmJGBmVmFOAmZmFeYkYGZWYU4CZmYV5iRgZlZh/x/qmymXfmch7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Histograma\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(141)\n",
    "plt.hist(evaluated_gradients[0])\n",
    "plt.title(\"Capa 1\")\n",
    "plt.subplot(142)\n",
    "plt.hist(evaluated_gradients[2])\n",
    "plt.title(\"Capa 2\")\n",
    "plt.subplot(143)\n",
    "plt.hist(evaluated_gradients[4])\n",
    "plt.title(\"Capa 3\")\n",
    "plt.subplot(144)\n",
    "plt.hist(evaluated_gradients[6])\n",
    "plt.title(\"Capa 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 402us/step - loss: nan - val_loss: nan\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: nan - val_loss: nan\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 4s 401us/step - loss: nan - val_loss: nan\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 3s 357us/step - loss: nan - val_loss: nan\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: nan - val_loss: nan\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: nan - val_loss: nan\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: nan - val_loss: nan\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: nan - val_loss: nan\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: nan - val_loss: nan\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: nan - val_loss: nan\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: nan - val_loss: nan\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: nan - val_loss: nan\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: nan - val_loss: nan\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: nan - val_loss: nan\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: nan - val_loss: nan\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: nan - val_loss: nan\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: nan - val_loss: nan\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 4s 374us/step - loss: nan - val_loss: nan\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: nan - val_loss: nan\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: nan - val_loss: nan\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: nan - val_loss: nan\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: nan - val_loss: nan\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: nan - val_loss: nan\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: nan - val_loss: nan\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: nan - val_loss: nan\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: nan - val_loss: nan\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: nan - val_loss: nan\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: nan - val_loss: nan\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: nan - val_loss: nan\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: nan - val_loss: nan\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 4s 378us/step - loss: nan - val_loss: nan\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: nan - val_loss: nan\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: nan - val_loss: nan\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: nan - val_loss: nan\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: nan - val_loss: nan\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: nan - val_loss: nan\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: nan - val_loss: nan\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: nan - val_loss: nan\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: nan - val_loss: nan\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: nan - val_loss: nan\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: nan - val_loss: nan\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: nan - val_loss: nan\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: nan - val_loss: nan\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: nan - val_loss: nan\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: nan - val_loss: nan\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: nan - val_loss: nan\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: nan - val_loss: nan\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: nan - val_loss: nan\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: nan - val_loss: nan\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: nan - val_loss: nan\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: nan - val_loss: nan\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: nan - val_loss: nan\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: nan - val_loss: nan\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: nan - val_loss: nan\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 359us/step - loss: nan - val_loss: nan\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: nan - val_loss: nan\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: nan - val_loss: nan\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: nan - val_loss: nan\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: nan - val_loss: nan\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: nan - val_loss: nan\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 4s 374us/step - loss: nan - val_loss: nan\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: nan - val_loss: nan\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: nan - val_loss: nan\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: nan - val_loss: nan\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: nan - val_loss: nan\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: nan - val_loss: nan\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: nan - val_loss: nan\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: nan - val_loss: nan\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: nan - val_loss: nan\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: nan - val_loss: nan\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: nan - val_loss: nan\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: nan - val_loss: nan\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: nan - val_loss: nan\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: nan - val_loss: nan\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: nan - val_loss: nan\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: nan - val_loss: nan\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: nan - val_loss: nan\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: nan - val_loss: nan\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: nan - val_loss: nan\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: nan - val_loss: nan\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: nan - val_loss: nan\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: nan - val_loss: nan\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: nan - val_loss: nan\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: nan - val_loss: nan\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: nan - val_loss: nan\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: nan - val_loss: nan\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: nan - val_loss: nan\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: nan - val_loss: nan\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: nan - val_loss: nan\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: nan - val_loss: nan\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: nan - val_loss: nan\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: nan - val_loss: nan\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: nan - val_loss: nan\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: nan - val_loss: nan\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: nan - val_loss: nan\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: nan - val_loss: nan\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: nan - val_loss: nan\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: nan - val_loss: nan\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: nan - val_loss: nan\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: nan - val_loss: nan\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: nan - val_loss: nan\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: nan - val_loss: nan\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: nan - val_loss: nan\n",
      "Epoch 163/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 3s 357us/step - loss: nan - val_loss: nan\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: nan - val_loss: nan\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: nan - val_loss: nan\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: nan - val_loss: nan\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: nan - val_loss: nan\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: nan - val_loss: nan\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: nan - val_loss: nan\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: nan - val_loss: nan\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: nan - val_loss: nan\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: nan - val_loss: nan\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 3s 357us/step - loss: nan - val_loss: nan\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: nan - val_loss: nan\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: nan - val_loss: nan\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: nan - val_loss: nan\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: nan - val_loss: nan\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: nan - val_loss: nan\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: nan - val_loss: nan\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 4s 359us/step - loss: nan - val_loss: nan\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: nan - val_loss: nan\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: nan - val_loss: nan\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: nan - val_loss: nan\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: nan - val_loss: nan\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: nan - val_loss: nan\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: nan - val_loss: nan\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: nan - val_loss: nan\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: nan - val_loss: nan\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: nan - val_loss: nan\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: nan - val_loss: nan\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: nan - val_loss: nan\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: nan - val_loss: nan\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: nan - val_loss: nan\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 4s 398us/step - loss: nan - val_loss: nan\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: nan - val_loss: nan\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: nan - val_loss: nan\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: nan - val_loss: nan\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: nan - val_loss: nan\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: nan - val_loss: nan\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: nan - val_loss: nan\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: nan - val_loss: nan\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: nan - val_loss: nan\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: nan - val_loss: nan\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: nan - val_loss: nan\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 4s 379us/step - loss: nan - val_loss: nan\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: nan - val_loss: nan\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: nan - val_loss: nan\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 4s 390us/step - loss: nan - val_loss: nan\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 4s 388us/step - loss: nan - val_loss: nan\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: nan - val_loss: nan\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: nan - val_loss: nan\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: nan - val_loss: nan\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: nan - val_loss: nan\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: nan - val_loss: nan\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: nan - val_loss: nan\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: nan - val_loss: nan\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: nan - val_loss: nan\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: nan - val_loss: nan\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: nan - val_loss: nan\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: nan - val_loss: nan\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: nan - val_loss: nan\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: nan - val_loss: nan\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: nan - val_loss: nan\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: nan - val_loss: nan\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: nan - val_loss: nan\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: nan - val_loss: nan\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 3s 357us/step - loss: nan - val_loss: nan\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "history = modelE.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtener gradientes luego de entrenar la red\n",
    "loss = keras.losses.mean_squared_error(modelE.output,y_train_scaled)\n",
    "listOfVariableTensors = modelE.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={modelE.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograma despues de entrenar\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(141)\n",
    "plt.hist(evaluated_gradients[0])\n",
    "plt.title(\"Capa 1\")\n",
    "plt.subplot(142)\n",
    "plt.hist(evaluated_gradients[2])\n",
    "plt.title(\"Capa 2\")\n",
    "plt.subplot(143)\n",
    "plt.hist(evaluated_gradients[4])\n",
    "plt.title(\"Capa 3\")\n",
    "plt.subplot(144)\n",
    "plt.hist(evaluated_gradients[6])\n",
    "plt.title(\"Capa 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e2)\n",
    "modelE = Sequential()\n",
    "modelE.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='relu'))\n",
    "modelE.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "modelE.add(Dense(256,  kernel_initializer='he_uniform',activation='relu'))\n",
    "modelE.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "modelE.add(Dense(256, kernel_initializer='he_uniform',activation='relu'))\n",
    "modelE.add(Dense(1, kernel_initializer='he_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelE.compile(optimizer=sgd,loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtener gradientes luego de entrenar la red\n",
    "loss = keras.losses.mean_squared_error(modelE.output,y_train_scaled)\n",
    "listOfVariableTensors = modelE.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={modelE.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograma\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(141)\n",
    "plt.hist(evaluated_gradients[0])\n",
    "plt.title(\"Capa 1\")\n",
    "plt.subplot(142)\n",
    "plt.hist(evaluated_gradients[2])\n",
    "plt.title(\"Capa 2\")\n",
    "plt.subplot(143)\n",
    "plt.hist(evaluated_gradients[4])\n",
    "plt.title(\"Capa 3\")\n",
    "plt.subplot(144)\n",
    "plt.hist(evaluated_gradients[6])\n",
    "plt.title(\"Capa 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = modelE.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtener gradientes luego de entrenar la red\n",
    "loss = keras.losses.mean_squared_error(modelE.output,y_train_scaled)\n",
    "listOfVariableTensors = modelE.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={modelE.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train_scaled) for gradient in evaluated_gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograma despues de entrenar\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(141)\n",
    "plt.hist(evaluated_gradients[0])\n",
    "plt.title(\"Capa 1\")\n",
    "plt.subplot(142)\n",
    "plt.hist(evaluated_gradients[2])\n",
    "plt.title(\"Capa 2\")\n",
    "plt.subplot(143)\n",
    "plt.hist(evaluated_gradients[4])\n",
    "plt.title(\"Capa 3\")\n",
    "plt.subplot(144)\n",
    "plt.hist(evaluated_gradients[6])\n",
    "plt.title(\"Capa 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f)\n",
    "modelf = Sequential()\n",
    "modelf.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelf.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "history = modelf.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))\n",
    "score = modelf.evaluate(X_test_scaled, y_test_scaled)\n",
    "print(\"\\n Loss: %.3f \\t:\" %(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 22s 2ms/step - loss: 83.0819 - val_loss: 14.2432\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 4s 387us/step - loss: 13.6692 - val_loss: 14.5544\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 13.6845 - val_loss: 14.6963\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 13.6880 - val_loss: 14.1150\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: 13.6888 - val_loss: 14.2681\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 13.6743 - val_loss: 14.9463\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 13.6794 - val_loss: 13.8457\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 13.6977 - val_loss: 13.9682\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 13.6940 - val_loss: 14.0755\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: 13.6852 - val_loss: 14.3462\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 4s 375us/step - loss: 13.6825 - val_loss: 14.0644\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 13.6856 - val_loss: 14.2408\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 13.7020 - val_loss: 14.6268\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 13.6784 - val_loss: 15.0816\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 13.6639 - val_loss: 14.7116\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: 13.6871 - val_loss: 14.4623\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 13.6838 - val_loss: 15.3920\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 13.6779 - val_loss: 14.8301\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 13.6868 - val_loss: 15.0782\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 13.6861 - val_loss: 14.0065\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: 13.6890 - val_loss: 14.3656\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 13.7098 - val_loss: 14.4430\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 13.6796 - val_loss: 14.5379\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 13.6803 - val_loss: 14.5178\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 13.6818 - val_loss: 14.9705\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 13.6654 - val_loss: 14.5725\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 13.6829 - val_loss: 14.7311\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 13.6748 - val_loss: 14.6808\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 13.6838 - val_loss: 14.5131\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 13.6638 - val_loss: 14.2845\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: 13.6841 - val_loss: 14.1338\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 13.6973 - val_loss: 14.2386\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 13.6831 - val_loss: 14.4575\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 13.6793 - val_loss: 14.5091\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 13.6741 - val_loss: 15.3975\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 13.6857 - val_loss: 14.2776\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 13.6946 - val_loss: 14.4321\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 13.6872 - val_loss: 14.2767\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 13.6951 - val_loss: 15.0656\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 13.6867 - val_loss: 14.1214\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 13.6893 - val_loss: 14.3802\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 13.6838 - val_loss: 14.0969\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 13.6883 - val_loss: 14.1225\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: 13.6834 - val_loss: 14.2660\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: 13.6706 - val_loss: 14.1336\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 13.6888 - val_loss: 15.1578\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 13.6966 - val_loss: 14.2696\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 13.6916 - val_loss: 14.8683\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 13.6859 - val_loss: 14.6632\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 13.6973 - val_loss: 14.2311\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 13.6846 - val_loss: 14.3377\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 13.6859 - val_loss: 13.9985\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 13.6968 - val_loss: 14.2144\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 13.6978 - val_loss: 14.2805\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: 13.6921 - val_loss: 14.7826\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 13.6771 - val_loss: 14.3338\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 13.6659 - val_loss: 14.0979\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 13.6875 - val_loss: 14.5006\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: 13.6945 - val_loss: 14.1097\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: 13.6813 - val_loss: 14.2930\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 13.6964 - val_loss: 14.4439\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 13.7002 - val_loss: 14.5719\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 13.6632 - val_loss: 14.3150\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 13.6742 - val_loss: 14.3541\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 4s 378us/step - loss: 13.6897 - val_loss: 14.3344\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 4s 379us/step - loss: 13.6767 - val_loss: 13.9871\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 13.6809 - val_loss: 14.5679\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 4s 444us/step - loss: 13.6981 - val_loss: 14.4733\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 4s 435us/step - loss: 13.6941 - val_loss: 14.7258\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 4s 388us/step - loss: 13.6784 - val_loss: 14.6053\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 3s 359us/step - loss: 13.6834 - val_loss: 14.4061\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 13.6672 - val_loss: 13.9606\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: 13.6809 - val_loss: 14.7318\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 13.6843 - val_loss: 14.4902\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 13.6835 - val_loss: 14.0963\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 13.7064 - val_loss: 14.7472\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 13.6830 - val_loss: 14.5937\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 13.6842 - val_loss: 14.6991\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 13.6875 - val_loss: 15.1316\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 13.6917 - val_loss: 15.8432\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 13.6893 - val_loss: 14.1554\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 13.6813 - val_loss: 14.1317\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 13.6922 - val_loss: 14.5215\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 13.6845 - val_loss: 15.2756\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 13.6759 - val_loss: 14.4317\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 13.6747 - val_loss: 14.4361\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 13.6866 - val_loss: 14.4307\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 13.6765 - val_loss: 15.1060\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 13.6912 - val_loss: 14.9129\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 13.6808 - val_loss: 14.6965\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 13.6999 - val_loss: 14.2879\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 13.6899 - val_loss: 14.9641\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 13.6928 - val_loss: 14.6876\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 13.7013 - val_loss: 15.0913\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 13.6829 - val_loss: 14.1091\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 13.6964 - val_loss: 14.0002\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 13.6814 - val_loss: 14.3907\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 13.6712 - val_loss: 14.1502\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 13.6834 - val_loss: 14.5544\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 13.7028 - val_loss: 14.9248\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 13.6846 - val_loss: 14.5336\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 13.6754 - val_loss: 14.1330\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 13.6832 - val_loss: 14.5866\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 13.6905 - val_loss: 14.0801\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: 13.6869 - val_loss: 14.5715\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 13.6823 - val_loss: 14.9037\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 13.6801 - val_loss: 14.5187\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 13.6929 - val_loss: 14.4182\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 4s 375us/step - loss: 13.6934 - val_loss: 14.0015\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 13.6953 - val_loss: 14.3609\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 13.7056 - val_loss: 14.3387\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 13.6726 - val_loss: 15.0517\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 13.6798 - val_loss: 14.3353\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 13.6754 - val_loss: 14.0252\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 13.6929 - val_loss: 14.8844\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 13.6864 - val_loss: 13.9273\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 13.6984 - val_loss: 14.6693\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 13.6933 - val_loss: 14.3643\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 13.6801 - val_loss: 14.6179\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 13.7066 - val_loss: 14.5499\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 13.6753 - val_loss: 15.0140\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 13.6932 - val_loss: 14.3826\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 13.6891 - val_loss: 14.2896\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 13.6921 - val_loss: 14.4895\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 13.6823 - val_loss: 14.1127\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 13.6744 - val_loss: 14.3132\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 13.6834 - val_loss: 15.1938\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 13.6806 - val_loss: 14.5237\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 13.6835 - val_loss: 14.7719\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 13.6929 - val_loss: 14.4358\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 13.7014 - val_loss: 14.1289\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 13.6950 - val_loss: 15.4454\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 13.6978 - val_loss: 14.9854\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 13.6725 - val_loss: 14.3410\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 13.6784 - val_loss: 14.8496\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 13.6862 - val_loss: 14.7526\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 13.6919 - val_loss: 14.3351\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 13.6931 - val_loss: 14.2383\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 13.6792 - val_loss: 13.9517\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 13.6576 - val_loss: 14.1956\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 13.6683 - val_loss: 14.1236\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 13.6736 - val_loss: 14.4762\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 13.6798 - val_loss: 14.3576\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 13.6743 - val_loss: 14.0115\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 13.6881 - val_loss: 14.1527\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 13.6879 - val_loss: 13.9739\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 13.6880 - val_loss: 13.9471\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 13.6927 - val_loss: 14.1298\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 13.6625 - val_loss: 14.6109\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 13.6788 - val_loss: 14.4065\n",
      "Epoch 151/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 3s 339us/step - loss: 13.6973 - val_loss: 14.4571\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 13.6738 - val_loss: 14.5350\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 13.6784 - val_loss: 15.2482\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 13.6792 - val_loss: 15.0080\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 4s 391us/step - loss: 13.6801 - val_loss: 14.5321\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 13.6672 - val_loss: 14.0953\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 13.6961 - val_loss: 14.1211\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 13.6846 - val_loss: 15.0003\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 13.6833 - val_loss: 14.3621\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 4s 378us/step - loss: 13.6817 - val_loss: 14.2156\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 13.6723 - val_loss: 14.1115\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 13.6942 - val_loss: 14.3337\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 13.6885 - val_loss: 15.3332\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 13.6979 - val_loss: 14.6086\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 13.6887 - val_loss: 14.7898\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 4s 389us/step - loss: 13.6765 - val_loss: 14.6803\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 13.6843 - val_loss: 14.6264\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 13.6848 - val_loss: 14.0098\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 5s 466us/step - loss: 13.6886 - val_loss: 14.4058\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 13.6831 - val_loss: 15.2089\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 13.6945 - val_loss: 14.6359\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 13.6694 - val_loss: 14.7320\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 13.6845 - val_loss: 14.2663\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: 13.6982 - val_loss: 14.2196\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 13.6761 - val_loss: 13.9933\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 13.6872 - val_loss: 13.8968\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 13.6786 - val_loss: 14.0259\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 4s 396us/step - loss: 13.6708 - val_loss: 14.3006\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 13.6832 - val_loss: 14.1825\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 13.6944 - val_loss: 14.7145\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 13.6925 - val_loss: 14.5156\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 13.6801 - val_loss: 14.1145\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 13.6940 - val_loss: 15.3024\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 13.6808 - val_loss: 14.9155\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 13.6914 - val_loss: 14.0192\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 13.6950 - val_loss: 14.3335\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 13.6895 - val_loss: 14.1223\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 13.6939 - val_loss: 14.0569\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: 13.6945 - val_loss: 14.7639\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 13.6787 - val_loss: 15.0293\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 13.6989 - val_loss: 14.5486\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 13.6789 - val_loss: 14.4850\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 13.6868 - val_loss: 14.5849\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 13.6861 - val_loss: 14.1119\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 13.6869 - val_loss: 14.0664\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 13.6977 - val_loss: 14.8027\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 13.6903 - val_loss: 15.1732\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 4s 390us/step - loss: 13.6897 - val_loss: 14.4953\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 4s 385us/step - loss: 13.6917 - val_loss: 14.5760\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 13.6817 - val_loss: 14.3702\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 13.6751 - val_loss: 14.5718\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 13.6865 - val_loss: 14.7185\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 4s 376us/step - loss: 13.6664 - val_loss: 14.9865\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 13.6849 - val_loss: 15.3271\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 13.7010 - val_loss: 14.4299\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 13.6843 - val_loss: 13.8461\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 13.6869 - val_loss: 14.3461\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 13.6867 - val_loss: 14.7333\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 13.6795 - val_loss: 15.0743\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 13.6896 - val_loss: 14.3897\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 13.6814 - val_loss: 14.6893\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 13.6775 - val_loss: 14.8381\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 4s 383us/step - loss: 13.6901 - val_loss: 13.9886\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: 13.6983 - val_loss: 14.6534\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 13.6803 - val_loss: 15.0757\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 13.6757 - val_loss: 14.9004\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 4s 380us/step - loss: 13.6948 - val_loss: 14.2042\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: 13.6792 - val_loss: 14.6820\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 13.6767 - val_loss: 15.0928\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 13.7044 - val_loss: 14.9378\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 13.6881 - val_loss: 15.2870\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 13.6843 - val_loss: 14.5837\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 13.6847 - val_loss: 14.3120\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 13.6807 - val_loss: 14.6601\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 13.6733 - val_loss: 14.3072\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: 13.6780 - val_loss: 14.5309\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 13.6758 - val_loss: 15.5881\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 13.6609 - val_loss: 14.0649\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 13.6915 - val_loss: 14.2881\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 13.6718 - val_loss: 14.0420\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 4s 379us/step - loss: 13.6811 - val_loss: 15.1515\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 13.6907 - val_loss: 14.2892\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 13.6998 - val_loss: 14.0420\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 13.6832 - val_loss: 14.5701\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 13.6698 - val_loss: 14.8892\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 4s 381us/step - loss: 13.6922 - val_loss: 14.3262\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: 13.6841 - val_loss: 13.8417\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 3s 342us/step - loss: 13.6814 - val_loss: 14.7489\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: 13.6862 - val_loss: 13.9370\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 4s 385us/step - loss: 13.6806 - val_loss: 15.0161\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 4s 385us/step - loss: 13.6785 - val_loss: 14.4282\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 13.6981 - val_loss: 14.5695\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 13.6759 - val_loss: 14.4974\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 13.6620 - val_loss: 16.2433\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 4s 378us/step - loss: 13.6842 - val_loss: 14.0228\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 13.6846 - val_loss: 14.1262\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 13.6944 - val_loss: 14.3457\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 336us/step - loss: 13.6742 - val_loss: 14.4851\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 13.6827 - val_loss: 14.3960\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 4s 372us/step - loss: 13.7001 - val_loss: 15.0554\n",
      "2437/2437 [==============================] - 0s 120us/step\n",
      "\n",
      " Loss: 10.452 \t:\n"
     ]
    }
   ],
   "source": [
    "modelf = Sequential()\n",
    "modelf.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.1)\n",
    "modelf.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "history = modelf.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))\n",
    "score = modelf.evaluate(X_test_scaled, y_test_scaled)\n",
    "print(\"\\n Loss: %.3f \\t:\" %(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 10s 986us/step - loss: 15.7905 - val_loss: 4.0707\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 3s 335us/step - loss: 0.9185 - val_loss: 0.4601\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 3s 333us/step - loss: 0.3924 - val_loss: 0.4907\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 3s 341us/step - loss: 0.2688 - val_loss: 0.2137\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.2339 - val_loss: 0.1811\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 0.2000 - val_loss: 0.1474\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.1529 - val_loss: 0.1211\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.1422 - val_loss: 0.1489\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.1330 - val_loss: 0.1488\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.1131 - val_loss: 0.1675\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.1170 - val_loss: 0.1083\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.1047 - val_loss: 0.0780\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.1043 - val_loss: 0.0989\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 0.0954 - val_loss: 0.1058\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0892 - val_loss: 0.0888\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0850 - val_loss: 0.1222\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0760 - val_loss: 0.1419\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.0744 - val_loss: 0.1085\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0701 - val_loss: 0.0788\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0671 - val_loss: 0.1075\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0713 - val_loss: 0.0694\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0664 - val_loss: 0.0844\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0610 - val_loss: 0.1093\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0725 - val_loss: 0.0878\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0585 - val_loss: 0.0790\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0564 - val_loss: 0.0575\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0562 - val_loss: 0.1784\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0560 - val_loss: 0.0623\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0531 - val_loss: 0.0613\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0524 - val_loss: 0.1086\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0480 - val_loss: 0.0615\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0470 - val_loss: 0.0602\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0455 - val_loss: 0.0579\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0453 - val_loss: 0.0640\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0431 - val_loss: 0.0582\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0450 - val_loss: 0.0532\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0393 - val_loss: 0.0488\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0398 - val_loss: 0.0593\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0399 - val_loss: 0.0487\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0403 - val_loss: 0.0659\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0373 - val_loss: 0.0481\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0415 - val_loss: 0.0565\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: 0.0359 - val_loss: 0.1220\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0374 - val_loss: 0.0423\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0395 - val_loss: 0.0578\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0355 - val_loss: 0.0501\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0369 - val_loss: 0.0673\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 3s 318us/step - loss: 0.0341 - val_loss: 0.0555\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0354 - val_loss: 0.0988\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0353 - val_loss: 0.0372\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0310 - val_loss: 0.1391\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 0.0336 - val_loss: 0.0355\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.0315 - val_loss: 0.0505\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0364 - val_loss: 0.0601\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0303 - val_loss: 0.0595\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0313 - val_loss: 0.0487\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0306 - val_loss: 0.0462\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0293 - val_loss: 0.0387\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0299 - val_loss: 0.0372\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0291 - val_loss: 0.0418\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0297 - val_loss: 0.0911\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0293 - val_loss: 0.0503\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0292 - val_loss: 0.0367\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0279 - val_loss: 0.0670\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0276 - val_loss: 0.0443\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0281 - val_loss: 0.0379\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0258 - val_loss: 0.0362\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0279 - val_loss: 0.0368\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0260 - val_loss: 0.0400\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0297 - val_loss: 0.0368\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0276 - val_loss: 0.0482\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0261 - val_loss: 0.0335\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 3s 306us/step - loss: 0.0265 - val_loss: 0.0378\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0261 - val_loss: 0.0428\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0256 - val_loss: 0.0365\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0244 - val_loss: 0.0396\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0243 - val_loss: 0.0359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0234 - val_loss: 0.0339\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0229 - val_loss: 0.0499\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0264 - val_loss: 0.0417\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0237 - val_loss: 0.0307\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0231 - val_loss: 0.0376\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0230 - val_loss: 0.0353\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0243 - val_loss: 0.0424\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0211 - val_loss: 0.0325\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0217 - val_loss: 0.0477\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0224 - val_loss: 0.0356\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0228 - val_loss: 0.0331\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0225 - val_loss: 0.0294\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 324us/step - loss: 0.0207 - val_loss: 0.0296\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0214 - val_loss: 0.0285\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0208 - val_loss: 0.0297\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0220 - val_loss: 0.0338\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0229 - val_loss: 0.0434\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0217 - val_loss: 0.0329\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0197 - val_loss: 0.0382\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0209 - val_loss: 0.0652\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0197 - val_loss: 0.0336\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0199 - val_loss: 0.0382\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0198 - val_loss: 0.0318\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0210 - val_loss: 0.0775\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0210 - val_loss: 0.0332\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0193 - val_loss: 0.0438\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0199 - val_loss: 0.0323\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.0206 - val_loss: 0.0318\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0197 - val_loss: 0.0412\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0229 - val_loss: 0.0308\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0204 - val_loss: 0.0679\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0193 - val_loss: 0.0435\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0203 - val_loss: 0.0303\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0186 - val_loss: 0.0292\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0182 - val_loss: 0.0271\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0203 - val_loss: 0.0412\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0198 - val_loss: 0.0314\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0185 - val_loss: 0.0335\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0183 - val_loss: 0.0282\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0193 - val_loss: 0.0854\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 3s 330us/step - loss: 0.0203 - val_loss: 0.0291\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0177 - val_loss: 0.0289\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0183 - val_loss: 0.0336\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0187 - val_loss: 0.0419\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 357us/step - loss: 0.0172 - val_loss: 0.0404\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 0.0181 - val_loss: 0.0270\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0177 - val_loss: 0.0273\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0175 - val_loss: 0.0288\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0167 - val_loss: 0.0360\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 337us/step - loss: 0.0180 - val_loss: 0.0280\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0172 - val_loss: 0.0267\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.0164 - val_loss: 0.0299\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0169 - val_loss: 0.0265\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0168 - val_loss: 0.0273\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 359us/step - loss: 0.0175 - val_loss: 0.0322\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0177 - val_loss: 0.0420\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0168 - val_loss: 0.0387\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0173 - val_loss: 0.0386\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0164 - val_loss: 0.0499\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 3s 334us/step - loss: 0.0163 - val_loss: 0.0307\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0175 - val_loss: 0.0435\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0166 - val_loss: 0.0536\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0169 - val_loss: 0.0303\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0170 - val_loss: 0.0306\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.0167 - val_loss: 0.0394\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0151 - val_loss: 0.0304\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0164 - val_loss: 0.0268\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0167 - val_loss: 0.0280\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0161 - val_loss: 0.0287\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 327us/step - loss: 0.0155 - val_loss: 0.0318\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0161 - val_loss: 0.0272\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0158 - val_loss: 0.0321\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0163 - val_loss: 0.0277\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.0158 - val_loss: 0.0360\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0163 - val_loss: 0.0293\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: 0.0161 - val_loss: 0.0275\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0152 - val_loss: 0.0263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0151 - val_loss: 0.0273\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0165 - val_loss: 0.0323\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.0149 - val_loss: 0.0437\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: 0.0162 - val_loss: 0.0346\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 4s 371us/step - loss: 0.0153 - val_loss: 0.0296\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0158 - val_loss: 0.0277\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 343us/step - loss: 0.0142 - val_loss: 0.0263\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 4s 381us/step - loss: 0.0145 - val_loss: 0.0445\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.0140 - val_loss: 0.0280\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0143 - val_loss: 0.0498\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0156 - val_loss: 0.0262\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0154 - val_loss: 0.0366\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0151 - val_loss: 0.0406\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0147 - val_loss: 0.0273\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0148 - val_loss: 0.0266\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0136 - val_loss: 0.0296\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0141 - val_loss: 0.0330\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0135 - val_loss: 0.0319\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0143 - val_loss: 0.0388\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.0139 - val_loss: 0.0252\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0140 - val_loss: 0.0292\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0136 - val_loss: 0.0560\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0143 - val_loss: 0.0281\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0137 - val_loss: 0.0264\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0149 - val_loss: 0.0332\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0143 - val_loss: 0.0280\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.0135 - val_loss: 0.0273\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0139 - val_loss: 0.0309\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0138 - val_loss: 0.0266\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0139 - val_loss: 0.0268\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.0139 - val_loss: 0.0263\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0130 - val_loss: 0.0279\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0133 - val_loss: 0.0372\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0139 - val_loss: 0.0421\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0133 - val_loss: 0.0396\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0132 - val_loss: 0.0270\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0131 - val_loss: 0.0371\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0128 - val_loss: 0.0285\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 3s 319us/step - loss: 0.0127 - val_loss: 0.0317\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.0129 - val_loss: 0.0301\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0132 - val_loss: 0.0271\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0136 - val_loss: 0.0518\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0127 - val_loss: 0.0283\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.0124 - val_loss: 0.0334\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 0.0132 - val_loss: 0.0296\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0125 - val_loss: 0.0398\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0136 - val_loss: 0.0339\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0130 - val_loss: 0.0259\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0124 - val_loss: 0.0280\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0124 - val_loss: 0.0272\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.0124 - val_loss: 0.0309\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0136 - val_loss: 0.0295\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0132 - val_loss: 0.0260\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0124 - val_loss: 0.0248\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 3s 328us/step - loss: 0.0127 - val_loss: 0.0316\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 0.0133 - val_loss: 0.0247\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0123 - val_loss: 0.0282\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0125 - val_loss: 0.0279\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0129 - val_loss: 0.0363\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 3s 325us/step - loss: 0.0129 - val_loss: 0.0300\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: 0.0121 - val_loss: 0.0310\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0118 - val_loss: 0.0253\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0119 - val_loss: 0.0241\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0121 - val_loss: 0.0292\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0118 - val_loss: 0.0360\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0118 - val_loss: 0.0248\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0122 - val_loss: 0.0310\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.0121 - val_loss: 0.0240\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0127 - val_loss: 0.0262\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0121 - val_loss: 0.0258\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 3s 340us/step - loss: 0.0124 - val_loss: 0.0308\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 3s 332us/step - loss: 0.0121 - val_loss: 0.0369\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0117 - val_loss: 0.0298\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0115 - val_loss: 0.0256\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0124 - val_loss: 0.0250\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 3s 339us/step - loss: 0.0111 - val_loss: 0.0310\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 3s 331us/step - loss: 0.0123 - val_loss: 0.0258\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0113 - val_loss: 0.0235\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0120 - val_loss: 0.0287\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0118 - val_loss: 0.0273\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 3s 329us/step - loss: 0.0112 - val_loss: 0.0357\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.0117 - val_loss: 0.0243\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 3s 301us/step - loss: 0.0125 - val_loss: 0.0258\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0120 - val_loss: 0.0247\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0116 - val_loss: 0.0406\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0122 - val_loss: 0.0261\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 0.0108 - val_loss: 0.0251\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0114 - val_loss: 0.0272\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0112 - val_loss: 0.0251\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0112 - val_loss: 0.0261\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0109 - val_loss: 0.0259\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.0108 - val_loss: 0.0329\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0113 - val_loss: 0.0247\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0114 - val_loss: 0.0266\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0105 - val_loss: 0.0265\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0107 - val_loss: 0.0311\n",
      "2437/2437 [==============================] - 0s 123us/step\n",
      "\n",
      " Loss: 0.041 \t:\n"
     ]
    }
   ],
   "source": [
    "modelf = Sequential()\n",
    "modelf.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='relu'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "modelf.add(Dense(256,  kernel_initializer='uniform',activation='relu'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='relu'))\n",
    "modelf.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelf.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "history = modelf.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))\n",
    "score = modelf.evaluate(X_test_scaled, y_test_scaled)\n",
    "print(\"\\n Loss: %.3f \\t:\" %(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 7s 723us/step - loss: 6.5295 - val_loss: 0.6051\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.5867 - val_loss: 0.3654\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 6s 579us/step - loss: 0.3431 - val_loss: 0.3782\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.2204 - val_loss: 0.1590\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 6s 584us/step - loss: 0.1557 - val_loss: 0.1545\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 5s 559us/step - loss: 0.1180 - val_loss: 0.1228\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 5s 539us/step - loss: 0.1017 - val_loss: 0.1583\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 6s 586us/step - loss: 0.0953 - val_loss: 0.0835\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 6s 579us/step - loss: 0.0787 - val_loss: 0.0890\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 6s 593us/step - loss: 0.0727 - val_loss: 0.0691\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 5s 505us/step - loss: 0.0686 - val_loss: 0.1340\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 5s 484us/step - loss: 0.0622 - val_loss: 0.0722\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 5s 498us/step - loss: 0.0608 - val_loss: 0.0686\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 5s 513us/step - loss: 0.0611 - val_loss: 0.0576\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 5s 495us/step - loss: 0.0529 - val_loss: 0.0609\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 5s 526us/step - loss: 0.0543 - val_loss: 0.0667\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 6s 596us/step - loss: 0.0474 - val_loss: 0.0517\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 6s 578us/step - loss: 0.0466 - val_loss: 0.0644\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 6s 571us/step - loss: 0.0478 - val_loss: 0.0517\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 6s 591us/step - loss: 0.0468 - val_loss: 0.0496\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 6s 575us/step - loss: 0.0453 - val_loss: 0.0558\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 6s 584us/step - loss: 0.0444 - val_loss: 0.0477\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 6s 587us/step - loss: 0.0408 - val_loss: 0.0465\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 6s 567us/step - loss: 0.0427 - val_loss: 0.1748\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 6s 603us/step - loss: 0.0399 - val_loss: 0.0446\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 6s 598us/step - loss: 0.0385 - val_loss: 0.0585\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 5s 553us/step - loss: 0.0405 - val_loss: 0.0446\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 6s 592us/step - loss: 0.0355 - val_loss: 0.0438\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 6s 595us/step - loss: 0.0388 - val_loss: 0.0549\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 5s 551us/step - loss: 0.0366 - val_loss: 0.0438\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 6s 589us/step - loss: 0.0328 - val_loss: 0.0504\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 6s 579us/step - loss: 0.0335 - val_loss: 0.0511\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 5s 556us/step - loss: 0.0323 - val_loss: 0.0455\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 6s 579us/step - loss: 0.0323 - val_loss: 0.0564\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 6s 578us/step - loss: 0.0324 - val_loss: 0.0496\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 5s 557us/step - loss: 0.0301 - val_loss: 0.0392\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 6s 576us/step - loss: 0.0306 - val_loss: 0.0387\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 5s 559us/step - loss: 0.0309 - val_loss: 0.0607\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 6s 585us/step - loss: 0.0313 - val_loss: 0.0423\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 6s 568us/step - loss: 0.0308 - val_loss: 0.0421\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 5s 548us/step - loss: 0.0277 - val_loss: 0.0468\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 6s 567us/step - loss: 0.0276 - val_loss: 0.0480\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 5s 552us/step - loss: 0.0271 - val_loss: 0.0440\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 5s 525us/step - loss: 0.0270 - val_loss: 0.0424\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0276 - val_loss: 0.0624\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.0300 - val_loss: 0.0456\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 5s 526us/step - loss: 0.0283 - val_loss: 0.0365\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0253 - val_loss: 0.0349\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 5s 551us/step - loss: 0.0291 - val_loss: 0.0580\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 5s 527us/step - loss: 0.0262 - val_loss: 0.0394\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 5s 536us/step - loss: 0.0256 - val_loss: 0.0381\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 5s 553us/step - loss: 0.0258 - val_loss: 0.0450\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 5s 529us/step - loss: 0.0248 - val_loss: 0.0329\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 5s 539us/step - loss: 0.0261 - val_loss: 0.0338\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 5s 541us/step - loss: 0.0238 - val_loss: 0.0374\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 5s 530us/step - loss: 0.0257 - val_loss: 0.0332\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 5s 533us/step - loss: 0.0236 - val_loss: 0.0357\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 5s 558us/step - loss: 0.0229 - val_loss: 0.0335\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 5s 527us/step - loss: 0.0246 - val_loss: 0.0515\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 5s 536us/step - loss: 0.0242 - val_loss: 0.0578\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 5s 541us/step - loss: 0.0239 - val_loss: 0.0431\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 5s 529us/step - loss: 0.0253 - val_loss: 0.0363\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 5s 530us/step - loss: 0.0227 - val_loss: 0.0395\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 5s 552us/step - loss: 0.0225 - val_loss: 0.0386\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 5s 532us/step - loss: 0.0216 - val_loss: 0.0338\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 5s 523us/step - loss: 0.0229 - val_loss: 0.0729\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 5s 543us/step - loss: 0.0244 - val_loss: 0.0331\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0224 - val_loss: 0.0345\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 5s 527us/step - loss: 0.0230 - val_loss: 0.0429\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 5s 556us/step - loss: 0.0246 - val_loss: 0.0396\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 5s 531us/step - loss: 0.0225 - val_loss: 0.0826\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0210 - val_loss: 0.0344\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0223 - val_loss: 0.0327\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 5s 540us/step - loss: 0.0207 - val_loss: 0.0319\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 5s 507us/step - loss: 0.0216 - val_loss: 0.0324\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 5s 549us/step - loss: 0.0210 - val_loss: 0.0472\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0184 - val_loss: 0.0350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 5s 516us/step - loss: 0.0190 - val_loss: 0.0414\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 5s 543us/step - loss: 0.0193 - val_loss: 0.0326\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 5s 543us/step - loss: 0.0198 - val_loss: 0.0338\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 5s 514us/step - loss: 0.0214 - val_loss: 0.0311\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 5s 553us/step - loss: 0.0190 - val_loss: 0.0320\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 5s 544us/step - loss: 0.0196 - val_loss: 0.0349\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 5s 519us/step - loss: 0.0193 - val_loss: 0.0299\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 5s 553us/step - loss: 0.0190 - val_loss: 0.0320\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 5s 544us/step - loss: 0.0200 - val_loss: 0.0365\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 5s 517us/step - loss: 0.0192 - val_loss: 0.0381\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 5s 554us/step - loss: 0.0181 - val_loss: 0.0341\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 5s 544us/step - loss: 0.0177 - val_loss: 0.0303\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 5s 517us/step - loss: 0.0196 - val_loss: 0.0511\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 5s 556us/step - loss: 0.0190 - val_loss: 0.0354\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 5s 550us/step - loss: 0.0176 - val_loss: 0.0350\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0181 - val_loss: 0.0324\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 5s 556us/step - loss: 0.0185 - val_loss: 0.0477\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 5s 548us/step - loss: 0.0191 - val_loss: 0.0300\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 5s 520us/step - loss: 0.0177 - val_loss: 0.0416\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 5s 551us/step - loss: 0.0174 - val_loss: 0.0348\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 5s 550us/step - loss: 0.0175 - val_loss: 0.0307\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 5s 523us/step - loss: 0.0172 - val_loss: 0.0413\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 5s 558us/step - loss: 0.0166 - val_loss: 0.0283\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 5s 545us/step - loss: 0.0178 - val_loss: 0.0329\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 6s 582us/step - loss: 0.0168 - val_loss: 0.0314\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 5s 556us/step - loss: 0.0163 - val_loss: 0.0404\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 6s 570us/step - loss: 0.0167 - val_loss: 0.0307\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 6s 570us/step - loss: 0.0166 - val_loss: 0.0280\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 5s 531us/step - loss: 0.0165 - val_loss: 0.0295\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 5s 489us/step - loss: 0.0164 - val_loss: 0.0388\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 5s 485us/step - loss: 0.0160 - val_loss: 0.0384\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 5s 499us/step - loss: 0.0168 - val_loss: 0.0304\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 5s 487us/step - loss: 0.0162 - val_loss: 0.0344\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 5s 487us/step - loss: 0.0163 - val_loss: 0.0517\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 5s 490us/step - loss: 0.0160 - val_loss: 0.0397\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 5s 491us/step - loss: 0.0157 - val_loss: 0.0359\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 5s 486us/step - loss: 0.0158 - val_loss: 0.0305\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 5s 484us/step - loss: 0.0167 - val_loss: 0.0273\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 5s 493us/step - loss: 0.0150 - val_loss: 0.0301\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 5s 487us/step - loss: 0.0157 - val_loss: 0.0314\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 5s 477us/step - loss: 0.0154 - val_loss: 0.0316\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 5s 492us/step - loss: 0.0154 - val_loss: 0.0289\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 5s 492us/step - loss: 0.0157 - val_loss: 0.0272\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 5s 486us/step - loss: 0.0148 - val_loss: 0.0437\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 5s 485us/step - loss: 0.0160 - val_loss: 0.0335\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 5s 487us/step - loss: 0.0153 - val_loss: 0.0306\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 5s 512us/step - loss: 0.0145 - val_loss: 0.0278\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 5s 488us/step - loss: 0.0144 - val_loss: 0.0289\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 5s 518us/step - loss: 0.0141 - val_loss: 0.0314\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 5s 508us/step - loss: 0.0140 - val_loss: 0.0264\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 5s 488us/step - loss: 0.0144 - val_loss: 0.0517\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 5s 511us/step - loss: 0.0145 - val_loss: 0.0282\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 5s 489us/step - loss: 0.0145 - val_loss: 0.0310\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 5s 486us/step - loss: 0.0148 - val_loss: 0.0301\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 5s 488us/step - loss: 0.0140 - val_loss: 0.0262\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 5s 488us/step - loss: 0.0144 - val_loss: 0.0272\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 5s 491us/step - loss: 0.0142 - val_loss: 0.0291\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 5s 476us/step - loss: 0.0134 - val_loss: 0.0316\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 5s 496us/step - loss: 0.0141 - val_loss: 0.0270\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 5s 491us/step - loss: 0.0140 - val_loss: 0.0305\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 5s 480us/step - loss: 0.0137 - val_loss: 0.0370\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 5s 493us/step - loss: 0.0143 - val_loss: 0.0328\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 5s 491us/step - loss: 0.0142 - val_loss: 0.0335\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 5s 484us/step - loss: 0.0133 - val_loss: 0.0257\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 5s 487us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 5s 493us/step - loss: 0.0132 - val_loss: 0.0299\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 5s 487us/step - loss: 0.0135 - val_loss: 0.0269\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 5s 474us/step - loss: 0.0140 - val_loss: 0.0279\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 5s 497us/step - loss: 0.0141 - val_loss: 0.0257\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 4s 429us/step - loss: 0.0136 - val_loss: 0.0324\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.0136 - val_loss: 0.0287\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 0.0124 - val_loss: 0.0270\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 3s 316us/step - loss: 0.0127 - val_loss: 0.0307\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0126 - val_loss: 0.0262\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0136 - val_loss: 0.0253\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0127 - val_loss: 0.0268\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0119 - val_loss: 0.0264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0126 - val_loss: 0.0269\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0122 - val_loss: 0.0271\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0126 - val_loss: 0.0285\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0125 - val_loss: 0.0258\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 3s 305us/step - loss: 0.0121 - val_loss: 0.0248\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0125 - val_loss: 0.0246\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 321us/step - loss: 0.0118 - val_loss: 0.0288\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 3s 314us/step - loss: 0.0119 - val_loss: 0.0251\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0121 - val_loss: 0.0282\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0121 - val_loss: 0.0275\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0127 - val_loss: 0.0263\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0119 - val_loss: 0.0270\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0123 - val_loss: 0.0239\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0119 - val_loss: 0.0264\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0114 - val_loss: 0.0253\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0117 - val_loss: 0.0244\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0113 - val_loss: 0.0283\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0115 - val_loss: 0.0264\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 311us/step - loss: 0.0125 - val_loss: 0.0253\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0118 - val_loss: 0.0283\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0107 - val_loss: 0.0275\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0127 - val_loss: 0.0248\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 3s 315us/step - loss: 0.0116 - val_loss: 0.0245\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0111 - val_loss: 0.0298\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0116 - val_loss: 0.0324\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0115 - val_loss: 0.0314\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0115 - val_loss: 0.0278\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0114 - val_loss: 0.0274\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 3s 322us/step - loss: 0.0116 - val_loss: 0.0246\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.0107 - val_loss: 0.0361\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0110 - val_loss: 0.0252\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0108 - val_loss: 0.0277\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0110 - val_loss: 0.0283\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0110 - val_loss: 0.0295\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0109 - val_loss: 0.0265\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0108 - val_loss: 0.0263\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0107 - val_loss: 0.0252\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0114 - val_loss: 0.0307\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 3s 309us/step - loss: 0.0116 - val_loss: 0.0269\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0110 - val_loss: 0.0255\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0109 - val_loss: 0.0252\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0105 - val_loss: 0.0253\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0104 - val_loss: 0.0305\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0115 - val_loss: 0.0255\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 3s 323us/step - loss: 0.0116 - val_loss: 0.0265\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 3s 312us/step - loss: 0.0106 - val_loss: 0.0251\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0107 - val_loss: 0.0359\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0119 - val_loss: 0.0239\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0107 - val_loss: 0.0236\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 3s 313us/step - loss: 0.0100 - val_loss: 0.0242\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 3s 320us/step - loss: 0.0112 - val_loss: 0.0296\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 3s 299us/step - loss: 0.0105 - val_loss: 0.0329\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 3s 293us/step - loss: 0.0110 - val_loss: 0.0240\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 3s 300us/step - loss: 0.0100 - val_loss: 0.0257\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 3s 304us/step - loss: 0.0105 - val_loss: 0.0238\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 3s 326us/step - loss: 0.0112 - val_loss: 0.0301\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 3s 310us/step - loss: 0.0109 - val_loss: 0.0269\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0105 - val_loss: 0.0240\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 3s 295us/step - loss: 0.0100 - val_loss: 0.0270\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 3s 296us/step - loss: 0.0102 - val_loss: 0.0233\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0101 - val_loss: 0.0241\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 4s 439us/step - loss: 0.0107 - val_loss: 0.0254\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.0098 - val_loss: 0.0298\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 3s 302us/step - loss: 0.0096 - val_loss: 0.0333\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 307us/step - loss: 0.0101 - val_loss: 0.0275\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 4s 413us/step - loss: 0.0107 - val_loss: 0.0286\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 4s 456us/step - loss: 0.0097 - val_loss: 0.0254\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 4s 460us/step - loss: 0.0104 - val_loss: 0.0273\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 4s 456us/step - loss: 0.0099 - val_loss: 0.0310\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 5s 462us/step - loss: 0.0102 - val_loss: 0.0245\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 4s 460us/step - loss: 0.0092 - val_loss: 0.0304\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 4s 460us/step - loss: 0.0101 - val_loss: 0.0242\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 5s 466us/step - loss: 0.0097 - val_loss: 0.0240\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 5s 463us/step - loss: 0.0100 - val_loss: 0.0281\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 4s 458us/step - loss: 0.0114 - val_loss: 0.0314\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 4s 450us/step - loss: 0.0097 - val_loss: 0.0257\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 5s 466us/step - loss: 0.0094 - val_loss: 0.0253\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 5s 465us/step - loss: 0.0091 - val_loss: 0.0251\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 5s 481us/step - loss: 0.0099 - val_loss: 0.0240\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.0090 - val_loss: 0.0292\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 4s 460us/step - loss: 0.0102 - val_loss: 0.0281\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 4s 459us/step - loss: 0.0094 - val_loss: 0.0239\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 4s 454us/step - loss: 0.0090 - val_loss: 0.0239\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 5s 466us/step - loss: 0.0092 - val_loss: 0.0256\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 4s 460us/step - loss: 0.0094 - val_loss: 0.0268\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 4s 455us/step - loss: 0.0097 - val_loss: 0.0250\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 5s 463us/step - loss: 0.0090 - val_loss: 0.0241\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 4s 461us/step - loss: 0.0098 - val_loss: 0.0230\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 4s 459us/step - loss: 0.0100 - val_loss: 0.0255\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 4s 457us/step - loss: 0.0096 - val_loss: 0.0239\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 5s 468us/step - loss: 0.0095 - val_loss: 0.0239\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 4s 454us/step - loss: 0.0093 - val_loss: 0.0323\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 4s 457us/step - loss: 0.0091 - val_loss: 0.0260\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 4s 461us/step - loss: 0.0095 - val_loss: 0.0280\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 5s 462us/step - loss: 0.0092 - val_loss: 0.0236\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 4s 456us/step - loss: 0.0093 - val_loss: 0.0279\n",
      "2437/2437 [==============================] - 6s 3ms/step\n",
      "\n",
      " Loss: 0.018 \t:\n"
     ]
    }
   ],
   "source": [
    "modelf = Sequential()\n",
    "modelf.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "modelf.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01, momentum=0.8)\n",
    "modelf.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "history = modelf.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))\n",
    "score = modelf.evaluate(X_test_scaled, y_test_scaled)\n",
    "print(\"\\n Loss: %.3f \\t:\" %(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g)\n",
    "modelf = Sequential()\n",
    "modelf.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='he_uniform',activation='softplus'))\n",
    "modelf.add(Dense(256, kernel_initializer='he_uniform',activation='softplus'))\n",
    "modelf.add(Dense(256,  kernel_initializer='he_uniform',activation='softplus'))\n",
    "modelf.add(Dense(256, kernel_initializer='he_uniform',activation='softplus'))\n",
    "modelf.add(Dense(256, kernel_initializer='he_uniform',activation='softplus'))\n",
    "modelf.add(Dense(1, kernel_initializer='he_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelf.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "history = modelf.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))\n",
    "score = modelf.evaluate(X_test_scaled, y_test_scaled)\n",
    "print(\"\\n Loss: %.3f \\t:\" %(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelf = Sequential()\n",
    "modelf.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='softplus'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='softplus'))\n",
    "modelf.add(Dense(256,  kernel_initializer='uniform',activation='softplus'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='softplus'))\n",
    "modelf.add(Dense(256, kernel_initializer='uniform',activation='softplus'))\n",
    "modelf.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "modelf.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "history = modelf.fit(X_train_scaled, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val_scaled))\n",
    "score = modelf.evaluate(X_test_scaled, y_test_scaled)\n",
    "print(\"\\n Loss: %.3f \\t:\" %(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h)\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "model.fit(X_train_scaled.values, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled.values, y_val_scaled))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
